{"url": "https://github.com/pfuehrlich-pik/GitHub-Action-Example", "owner": "pfuehrlich-pik", "repository_name": "GitHub-Action-Example", "date_all_variable_collection": "2023-07-04", "description": null, "size": 1, "stargazers_count": 0, "watchers_count": 0, "language": "R", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "pfuehrlich-pik", "contributions": 3}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 352}]}
{"url": "https://github.com/0UmfHxcvx5J7JoaOhFSs5mncnisTJJ6q/moinput", "owner": "0UmfHxcvx5J7JoaOhFSs5mncnisTJJ6q", "repository_name": "moinput", "date_all_variable_collection": "2023-07-04", "description": "Provides useful functions and a common structure to all the input data required to run models like MAgPIE and REMIND     of model input data.", "size": 1185, "stargazers_count": 0, "watchers_count": 0, "language": "R", "has_issues": false, "has_projects": true, "has_downloads": true, "has_wiki": false, "has_pages": false, "has_discussions": false, "forks_count": 12, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "GNU Lesser General Public License v3.0", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 12, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "Loisel", "contributions": 26}, {"contributor": "bodirsky", "contributions": 20}, {"contributor": "k4rst3ns", "contributions": 20}, {"contributor": "caviddhen", "contributions": 16}, {"contributor": "0UmfHxcvx5J7JoaOhFSs5mncnisTJJ6q", "contributions": 15}, {"contributor": "amnmalik", "contributions": 14}, {"contributor": "LaviniaBaumstark", "contributions": 10}, {"contributor": "wehnerja", "contributions": 9}, {"contributor": "dklein-pik", "contributions": 6}, {"contributor": "johanneskoch94", "contributions": 6}, {"contributor": "giannou", "contributions": 6}, {"contributor": "abhimishr", "contributions": 4}, {"contributor": "fschreyer", "contributions": 4}, {"contributor": "weindl", "contributions": 3}, {"contributor": "Renato-Rodrigues", "contributions": 3}, {"contributor": "emolinab", "contributions": 3}, {"contributor": "christophbertram", "contributions": 2}, {"contributor": "tscheypidi", "contributions": 1}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 2037685}], "readme": "# R moinput package\n\n## Purpose and Functionality\n\nThe R-library moinput provides useful functions and a common structure to all the input data required to run models like MAgPIE and REMIND\n\n\n## Installation\n\nFor installation of the most recent package version an additional repository has to be added in R:\n\n```r\noptions(repos = c(CRAN = \"@CRAN@\", rd3mod_repo = \"http://www.pik-potsdam.de/rd3mod/R/\"))\n```\nThe additional repository can be made available permanently by adding the line above to a file called `.Rprofile` stored in the home folder of your system (`Sys.glob(\"~\")` in R returns the home directory).\n\nAfter that the most recent version of the package can be installed using `install.packages`:\n\n```r \ninstall.packages(\"moinput\")\n```\n\nPackage updates can be installed using `update.packages` (make sure that the additional repository has been added before running that command):\n\n```r \nupdate.packages()\n```\n\n## Tutorial\n\nThe package comes with a vignette describing the basic functionality of the package and how to use it. You can load it with the following command (the package needs to be installed):\n\n```r \nvignette(\"moinput\")\n```\n\n\n## Questions / Problems\n\nIn case of questions / problems please contact Jan Dietrich <dietrich@pik-potsdam.de>.\n\n\n"}
{"url": "https://github.com/0UmfHxcvx5J7JoaOhFSs5mncnisTJJ6q/testing", "owner": "0UmfHxcvx5J7JoaOhFSs5mncnisTJJ6q", "repository_name": "testing", "date_all_variable_collection": "2023-07-04", "description": null, "size": 0, "stargazers_count": 0, "watchers_count": 0, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": []}
{"url": "https://github.com/atreyasha/clingo-solvers", "owner": "atreyasha", "repository_name": "clingo-solvers", "date_all_variable_collection": "2023-07-04", "description": "Answer Set Programming (ASP): clingo solvers for NxN Sudoku, Yosenabe, Minotaur and Elevator games", "size": 676, "stargazers_count": 6, "watchers_count": 6, "language": "Prolog", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 1, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 1, "open_issues": 0, "watchers": 6, "default_branch": "main", "contributors": [{"contributor": "atreyasha", "contributions": 56}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Prolog", "num_chars": 144291}], "readme": "# clingo-solvers\n\nThis repository contains NxN-Sudoku, Yosenabe, Minotaur and Elevator game solvers implemented in [clingo](https://github.com/potassco/clingo). The report for the final elevator project can be found [here](./docs/asp_elevator.pdf).\n\n## Dependencies\n\nInstall `clingo` onto your system, either via your package manager (recommended, if available) or [building from source](https://github.com/potassco/clingo/blob/master/INSTALL.md).\n\nThe source code in this repository was tested against `v5.3.0` of `clingo`.\n\n## Usage\n\nSource code for `clingo` solvers is available in the `src` directory. The `test` directory contains test instances of the respective games.\n\nAs an example, one can test the default 9x9 sudoku solver by executing:\n\n```shell\n$ clingo ./test/test_sudoku.lp ./src/sudoku.lp 0\n```\n\n## Known issue(s)\n\nBased on local experiments, all solvers except the Minotaur solver in `minotaur.lp` and/or `minotaur_alternative.lp` succeeded in Yeti benchmark tests. Pull requests for an improved Minotaur solver are therefore very welcome.\n"}
{"url": "https://github.com/atreyasha/deep-generative-models", "owner": "atreyasha", "repository_name": "deep-generative-models", "date_all_variable_collection": "2023-07-04", "description": "Deep generative models implemented with TensorFlow 2.0: eg. Restricted Boltzmann Machine (RBM), Deep Belief Network (DBN), Deep Boltzmann Machine (DBM), Convolutional Variational Auto-Encoder (CVAE), Convolutional Generative Adversarial Network (CGAN)", "size": 27191, "stargazers_count": 1, "watchers_count": 1, "language": "Python", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 1, "default_branch": "main", "contributors": [{"contributor": "atreyasha", "contributions": 96}, {"contributor": "dependabot[bot]", "contributions": 2}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Python", "num_chars": 77983}, {"language": "Shell", "num_chars": 3083}], "readme": "## Deep Generative Models using TensorFlow 2.0\n\nThis repository contains TensorFlow 2.0 python source-code for deep generative models. This repository prioritizes using low-level tensorflow implementations where possible. The intended purpose is to allow for a more in-depth understanding of corresponding algorithms.\n\n**Note:** Source code in this repository was only tested on the CPU.\n\n### Dependencies\n\nThis repository's source code was tested with Python versions `3.7.*`.\n\n1. Install python dependencies located in `requirements.txt`:\n\n    ```shell\n    $ pip install -r requirements.txt\n    ```\n\n2. **Optional:** To develop this repository, it is recommended to initialize a pre-commit hook for automatic updates of python dependencies:\n\n    ```shell\n    $ ./init.sh\n    ```\n\n### Workflow\n\nInformation regarding execution of python scripts can be found in the [readme](/src/README.md) in the `/src` directory.\n"}
{"url": "https://github.com/atreyasha/dotfiles", "owner": "atreyasha", "repository_name": "dotfiles", "date_all_variable_collection": "2023-07-04", "description": "Dotfiles for a minimal Arch Linux build managed with GNU Stow. Packages include i3-gaps, emacs, neomutt and many more", "size": 7869, "stargazers_count": 0, "watchers_count": 0, "language": "Emacs Lisp", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "atreyasha", "contributions": 580}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Emacs Lisp", "num_chars": 50580}, {"language": "Shell", "num_chars": 36212}, {"language": "Python", "num_chars": 6696}, {"language": "Vim Script", "num_chars": 2776}, {"language": "Perl", "num_chars": 1613}, {"language": "Makefile", "num_chars": 1031}, {"language": "R", "num_chars": 86}], "readme": "# dotfiles\n\nThis repository contains dotfiles for a minimal Arch Linux build called [`monix`](https://github.com/atreyasha/monix). Dotfiles include `i3-gaps`, `emacs`, `neomutt` and many more. Below is a screenshot of a sample workspace:\n\n<p align=\"center\">\n<img src=\"./docs/screenshot.png\" width=\"700\">\n</p>\n\n## Initialization\n\nClone this repository in your `$HOME` directory. Next, pull all submodules and initialize a pre-commit hook by executing:\n\n```\n$ make init\n```\n\n## Installation\n\nThis repository uses [GNU Stow](https://www.gnu.org/software/stow/) for managing dotfiles and respective symbolic links.\n\nTo deploy all dotfiles, execute:\n\n```\n$ make install\n```\n\nTo deploy an opinionated subset of dotfiles for a remote machine, execute:\n\n```\n$ make install.remote\n```\n\n## Test\n\nTo test the deployment of `dotfiles`, execute:\n\n```\n$ make test\n```\n\n## Uninstallation\n\nTo remove all stowed `dotfiles`, execute:\n\n```\n$ make uninstall\n```\n\n## Development\n\nCheck out our development [log](./docs/develop.md) for details on upcoming developments.\n"}
{"url": "https://github.com/atreyasha/git-hooks", "owner": "atreyasha", "repository_name": "git-hooks", "date_all_variable_collection": "2023-07-04", "description": "Pre and post-commit git hooks optimized for python, shell, R and org-mode development workflows; as well as secondary branch rebasing", "size": 36, "stargazers_count": 0, "watchers_count": 0, "language": "Shell", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "atreyasha", "contributions": 46}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Shell", "num_chars": 4345}], "readme": "# git-hooks :anchor:\n\nThis repository documents git hooks which assist with `python`, `shell`, `R` and `org-mode` development workflows as well as secondary branch rebasing.\n\n## Overview :book:\n\n<details><summary>Pre-commit hook</summary><p>\n\n`pre-commit` contains a useful hook which is, from its name, a workflow that is executed before every commit. The various functions and dependencies in the shell script are described below:\n\n| Function                   | Description                                                                                                          | Dependencies                                      |\n|:---------------------------|:---------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|\n| update_python_dependencies | Updates `requirements.txt` to maintain log of python dependencies                                                    | [poetry](https://github.com/python-poetry/poetry) |\n| lint_shell_scripts         | Lints all shell scripts with consistent indents                                                                      | [shfmt](https://github.com/mvdan/sh)              |\n| lint_R_scripts             | Lints all R scripts for clean code                                                                                   | [styler](https://github.com/r-lib/styler)         |\n| convert_org_to_md          | Converts specified `org` file(s) to github-flavored `markdown`, adds a `TOC` and cleans up `TODO` and `DONE` markers | [pandoc](https://github.com/jgm/pandoc)           |\n\nIn addition, we provide a `main` function where the user can decide which of the above functions to use.\n\n</p></details>\n<details><summary>Post-commit hook</summary><p>\n\n`post-commit` contains a simpler hook which keeps specified secondary branches rebased with a primary branch. Here, we provide only one function:\n\n| Function      | Description                                                                                                                                          | Dependencies |\n|:--------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------|:-------------|\n| rebase_branch | Rebases a secondary branch with a primary branch. This could be useful to keep one branch up-to-date with another while still offering new features. | -            |\n\nThe names of the aforementioned branches can be specified in the `main` function.\n\n</p></details>\n\n## Usage :snowflake:\n\n1. Edit the `main` function(s) of the hooks to customize callable functions and input parameters.\n\n2. In order to initialize both hooks, copy the edited hooks to `./.git/hooks/` in your desired `git` repository. For example:\n\n    ```shell\n    $ cp /path/to/pre-commit ./.git/hooks/\n    $ cp /path/to/post-commit ./.git/hooks/\n    ```\n\n**Note:** These hooks are generally non-invasive, ie. they exit gracefully if dependencies or staged changes are missing and do not interfere with the overall commit process in case of failures.\n\n## Bugs/Issues :bug:\n\nIn case of bugs or suggestions for improvements, feel free to open a GitHub issue.\n"}
{"url": "https://github.com/atreyasha/i3-balance-workspace", "owner": "atreyasha", "repository_name": "i3-balance-workspace", "date_all_variable_collection": "2023-07-04", "description": "Balance windows and workspaces in i3wm", "size": 12334, "stargazers_count": 11, "watchers_count": 11, "language": "Python", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 1, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 1, "watchers": 11, "default_branch": "main", "contributors": [{"contributor": "atreyasha", "contributions": 121}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": true, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Python", "num_chars": 22571}, {"language": "Shell", "num_chars": 3150}, {"language": "Makefile", "num_chars": 1342}], "readme": "# i3-balance-workspace\n\n[![GitHub tag (latest SemVer)](https://img.shields.io/github/v/tag/atreyasha/i3-balance-workspace?color=brightgreen&label=release&logo=GitHub)](https://github.com/atreyasha/i3-balance-workspace/tags)\n[![PyPI](https://img.shields.io/pypi/v/i3-balance-workspace?color=brightgreen&logo=pypi&logoColor=yellow)](https://pypi.org/project/i3-balance-workspace/)\n[![AUR version](https://img.shields.io/aur/version/i3-balance-workspace?color=brightgreen&logo=Arch%20Linux)](https://aur.archlinux.org/packages/i3-balance-workspace/)\n\nBalance windows and workspaces in i3wm. Functionality is similar to the `Emacs` command `M-x balance-windows`.\n\n## Installation\n\nFollowing are available options to install `i3-balance-workspace`:\n\n1. Install from PyPi (Python Package Index) using `pip`:\n\n    ```shell\n    $ pip install i3-balance-workspace\n    ```\n\n2. For Arch-Linux users, install `i3-balance-workspace` via the [AUR](https://aur.archlinux.org/packages/i3-balance-workspace/).\n\n3. To install locally, ensure `poetry` and `pip` are installed on your system. Then execute:\n\n    ```shell\n    $ make install\n    ```\n\n## Usage\n\n```\nusage: i3_balance_workspace [-h] [--scope {workspace,focus}] [--timeout <int>]\n\noptions:\n  --scope     {workspace,focus}\n              scope of resizing containers (default: workspace)\n  --timeout   <int>\n              timeout in seconds for resizing (default: 1)\n  -h, --help  <flag>\n              show this help message and exit\n```\n\nIn order to balance all windows in the current workspace, simply execute:\n\n```shell\n$ i3_balance_workspace\n```\n\nAlternatively, it is possible to only balance the windows that are in focus. For this, execute the following:\n\n```shell\n$ i3_balance_workspace --scope focus\n```\n\nIn order to get the full benefit of this routine, it is recommended to initialize i3 persistent keybindings. Below are example keybindings which can be appended to your i3 `config` file.\n\n```shell\nbindsym $mod+b exec \"i3_balance_workspace --scope focus\"\nbindsym $mod+Shift+b exec \"i3_balance_workspace\"\n```\n\n## Examples\n\n`i3-balance-workspace` has been tested and shows good performance on both simple and complex workspace layouts. Take a look at some examples:\n\n### Scope: Workspace\n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/atreyasha/i3-balance-workspace/main/docs/workspace.gif\" width=\"800\">\n</p>\n\n### Scope: Focused windows\n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/atreyasha/i3-balance-workspace/main/docs/windows.gif\" width=\"800\">\n</p>\n\n## Bugs\n\nIn case of any bugs, feel free to open a GitHub issue.\n\n## Developments\n\nFurther developments to this repository are summarized in our development [log](https://github.com/atreyasha/i3-balance-workspace/blob/main/docs/develop.md).\n"}
{"url": "https://github.com/atreyasha/language-detection", "owner": "atreyasha", "repository_name": "language-detection", "date_all_variable_collection": "2023-07-04", "description": "Language detection for WiLI-2018 via character n-gram profiles from Cavnar and Trenkle (1994)", "size": 1035, "stargazers_count": 0, "watchers_count": 0, "language": "Python", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 1, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 1, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "atreyasha", "contributions": 38}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Python", "num_chars": 22845}, {"language": "Shell", "num_chars": 4813}], "readme": "# language-detection\n\nThis repository provides a workflow for language detection using a character n-gram language profiling technique from Cavnar and Trenkle (1994). The WiLI-2018 language identification data set (235 languages) is used for both training and evaluation. Our best language detection model achieved a weighted F<sub>1</sub> score of 89.8% on the WiLI-2018 test set. A presentation summarizing this project can be found [here](./docs/presentation/main.pdf).\n\n## Dependencies :neckbeard:\n\nThis repository's code was tested with Python versions `3.7.*`. To sync dependencies, we recommend creating a virtual environment and installing the relevant packages via `pip`:\n\n```\n$ pip install -r requirements.txt\n```\n\n## Initialization :fire:\n\n1. Automatically download and prepare the WiLI-2018 [data set](https://zenodo.org/record/841984):\n\n    ```\n    $ bash scripts/prepare_data.sh\n    ```\n\n2. **Optional:** Initialize git hooks to manage development workflows such as linting shell scripts, keeping python dependencies up-to-date and formatting the development log:\n\n    ```\n    $ bash scripts/prepare_git_hooks.sh\n    ```\n\n## Usage :snowflake:\n\n<details><summary>i. Training</summary>\n<p>\n\n```\nusage: train.py [-h] [--logging-level {debug,info,warning,error,critical}]\n                [--models-directory <dir_path>] [--ngram-cutoff <int>]\n                [--ngrams <int>] [--train-data <file_path>]\n                [--train-labels <file_path>]\n\noptional arguments:\n  --logging-level     {debug,info,warning,error,critical}\n                      Set logging level (default: info)\n  --models-directory  <dir_path>\n                      Directory to dump models and logs (default: ./models)\n  --ngram-cutoff      <int>\n                      Maximum character n-grams per language profile (default:\n                      300)\n  --ngrams            <int>\n                      Character n-grams to use for language profiles (default:\n                      3)\n  --train-data        <file_path>\n                      Path to training data (default:\n                      ./data/wili-2018/x_train.txt)\n  --train-labels      <file_path>\n                      Path to training labels (default:\n                      ./data/wili-2018/y_train.txt)\n  -h, --help          show this help message and exit\n```\n\nTo train a language detection model using our defaults, simply execute:\n\n```\n$ python3 -m src.train\n```\n\n**Note:** Our default model is already provided in the `./models` directory\n\n</p>\n</details>\n\n<details><summary>ii. Evaluation</summary>\n<p>\n\n```\nusage: evaluate.py [-h] [--logging-level {debug,info,warning,error,critical}]\n                   [--model <file_path>] [--models-directory <dir_path>]\n                   [--test-data <file_path>] [--test-labels <file_path>]\n\noptional arguments:\n  --logging-level     {debug,info,warning,error,critical}\n                      Set logging level (default: info)\n  --model             <file_path>\n                      Path to model JSON file (default:\n                      ./models/model_3_300.json)\n  --models-directory  <dir_path>\n                      Directory to dump models and logs (default: ./models)\n  --test-data         <file_path>\n                      Path to test data (default: ./data/wili-2018/x_test.txt)\n  --test-labels       <file_path>\n                      Path to test labels (default:\n                      ./data/wili-2018/y_test.txt)\n  -h, --help          show this help message and exit\n```\n\nTo evaluate the default language detection model, simply execute:\n\n```\n$ python3 -m src.evaluate\n```\n\nThis will dump a classification report into the directory specified in `--models-directory`.\n\n**Note:** The classification report for our default model is already provided in the `./models` directory\n\n</p>\n</details>\n\n<details><summary>iii. Prediction</summary>\n<p>\n\n```\nusage: predict.py [-h] --predict-data <file_path>\n                  [--logging-level {debug,info,warning,error,critical}]\n                  [--model <file_path>]\n\noptional arguments:\n  --logging-level  {debug,info,warning,error,critical}\n                   Set logging level (default: info)\n  --model          <file_path>\n                   Path to model JSON file (default:\n                   ./models/model_3_300.json)\n  -h, --help       show this help message and exit\n\nrequired arguments:\n  --predict-data   <file_path>\n                   Path to prediction data (default: None)\n```\n\nTo predict the language of a document using our default language detection model, simply execute:\n\n```\n$ python3 -m src.predict --predict-data /path/to/document\n```\n\n**Notes:**\n\n1. This prediction workflow assumes one document per line. If your document is multi-lined, please condense it into a single line\n\n2. The meaning of each output label is expounded in `./data/wili-2018/labels.csv`\n\n</p>\n</details>\n\n## Test :microscope:\n\n1. To run a `mypy` typecheck on our source code, execute:\n\n    ```\n    $ bash scripts/test_typecheck.sh\n    ```\n\n2. To test our default model on some sample data, execute the following:\n\n    ```\n    $ python3 -m src.predict --predict-data test/input.txt\n    ```\n\n## References :book:\n\n```bibtex\n@inproceedings{cavnar1994n,\n  title={N-gram-based text categorization},\n  author={Cavnar, William B and Trenkle, John M and others},\n  booktitle={Proceedings of SDAIR-94, 3rd annual symposium on\n  document analysis and information retrieval},\n  volume={161175},\n  year={1994},\n  organization={Citeseer}\n}\n\n@article{thoma2018wili,\n  title={The WiLI benchmark dataset for written language identification},\n  author={Thoma, Martin},\n  journal={arXiv preprint arXiv:1801.07779},\n  year={2018}\n}\n```\n\n<!--  LocalWords:  Cavnar Trenkle WiLI neckbeard typecheck\n -->\n"}
{"url": "https://github.com/atreyasha/memory-daemon", "owner": "atreyasha", "repository_name": "memory-daemon", "date_all_variable_collection": "2023-07-04", "description": "Memory-daemon for remote RAM management with email alerts", "size": 116, "stargazers_count": 1, "watchers_count": 1, "language": "Shell", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 1, "default_branch": "main", "contributors": [{"contributor": "atreyasha", "contributions": 58}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Shell", "num_chars": 4069}, {"language": "Makefile", "num_chars": 651}], "readme": "## memory-daemon\n\nThis project documents a memory-daemon cron service that tracks the used RAM in a server and terminates a dominant user process if a defined used RAM threshold is exceeded. Additionally, this service sends an email to the user to inform her/him about the termination of a memory-consuming service.\n\nThis service is not meant as a replacement for server management programs, but is rather meant for situations where users want to manage their own memory consuming processes to prevent annoying server crashes.\n\n**Important:** This service only has permissions to terminate processes from the user which installed/uses it. It cannot terminate other users' processes. In the case that another user's process results in high RAM usage, an email will be sent to registered users warning them about the high RAM usage.\n\n### Installation\n\n1. Install `mem_daemon`:\n\n    ```\n    $ make install\n    ```\n\n2. If all goes well, edit the default configuration file `~/.config/mem_daemon/md.conf`\n\n    The following options must be configured in order to use this service:\n\n    a. `receiver`: receiver's email address regarding RAM warnings/termination notifications. Multiple emails can be configured if they are all comma-separated.\n\n    b. `sender`: the sender's email address (a dummy one can be made with an email service)\n\n    c. `pass`: plaintext password for the sender's email address (poses security risk; advisable to use a non-critical email address)\n\n    d. `threshold`: percentage of total memory used must cross this `integer` threshold in order to trigger the memory-daemon into killing the most ram intensive process and sending the user an email\n\n    e. `smtp`: smtp address of the SMTP server for the sender's email provider\n\n    f. `port`: port of the above-defined smtp address\n\n    An example of a valid `md.conf` is shown below:\n\n    ```\n    receiver receiver@somemail.com\n    sender sender@anothermail.com\n    pass somepass\n    threshold 90\n    smtp smtp.anothermail.com\n    port 587\n    ```\n\n3. Once all the above options have been configured, we can test the service. An email will be sent to your specified account and no process will be killed. Please check your spam folder in case nothing appears. For this, execute the following:\n\n    ```\n    $ make test\n    ```\n\n    Below is an example of the dummy email:\n    \n    <kbd>\n    <img src=\"/img/screenshot.png\" width=\"600\">\n    </kbd>\n\n4. Finally, in order to set up the memory-daemon as a regular service, we would need to install a `crontab` for it. You will be prompted to input the periodicity (in minutes) with which the memory-daemon checks your server. Execute the following: \n\n    ```\n    $ make cronjob\n    ```\n    \n    **Troubleshooting:** If an error is thrown that crontabs are not installed for the user, simply run `crontab -e`. Then, a prompt should appear requesting for the text editor that should be used to edit the crontab, for which you can choose your most preferred text editor. Next, you can enter a dummy crontab (which prints `hello world`) to initialize the service, such as:\n    \n    ```\n    *\\1 * * * * /bin/echo \"hello world\"\n    ```\n    \n    Then, exit the editor and the following output should be received: `crontab: installing user crontab`. Now, you can proceed back with step 4. You can also safely remove the line containing the `/bin/echo` command after step 4.\n\n    Now, the memory-daemon is activated. In order to test out its utility, you can manually set the threshold in `~/.config/mem_daemon/md.conf` to a low value, such as `30`. Then, run a RAM intensive script and check if it gets terminated by `mem_daemon` and if you receive an email notification about it.\n\n    **Note:** The output of the crontab will be appended to `~/.config/mem_daemon/md.log` for debugging purposes.\n\n### Uninstallation\n\nIn order to uninstall `mem_daemon`, execute the following:\n\n```\n$ make uninstall\n```\n\nThis will remove the transferred executables, as well as any installed `crontab`.\n"}
{"url": "https://github.com/atreyasha/mimic3-benchmarks-occlusion", "owner": "atreyasha", "repository_name": "mimic3-benchmarks-occlusion", "date_all_variable_collection": "2023-07-04", "description": "Perturbation analysis of MIMIC-III Benchmarks best IHM model using occlusion", "size": 1685, "stargazers_count": 0, "watchers_count": 0, "language": "Python", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 1, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 1, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "atreyasha", "contributions": 26}, {"contributor": "dependabot[bot]", "contributions": 1}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Python", "num_chars": 79715}, {"language": "R", "num_chars": 10862}, {"language": "Shell", "num_chars": 3583}], "readme": "## MIMIC-III Benchmarks IHM Occlusion Analysis\n\nExplainability in AI is a crucial research field, particularly in the biomedical domain where results derived from machine-learning models could help to make important decisions for hospitals.\n\nIn this repository, we conduct an occlusion (perturbation) analysis of the MIMIC-III benchmarks best in-hospital-mortality (IHM) model. The MIMIC-III benchmark model(s) are detailed in an upstream repository [here](https://github.com/YerevaNN/mimic3-benchmarks), which is distributed under the MIT [License](./THIRD_PARTY_NOTICES.txt).\n\n### Table of Contents\n1. [Dependencies](#1-Dependencies) \n2. [Repository initialization](#2-Repository-initialization)\n3. [Occlusion pipeline](#3-Occlusion-pipeline)\n4. [Visualization](#4-Visualization)\n5. [Interpretation](#5-Interpretation)\n6. [Reference](#6-Reference)\n\n### 1. Dependencies\n\n**i.** In order to set up this repository, we would need to satisfy local pythonic dependencies. If `poetry` is installed on your system, you can install dependencies and create a virtual environment automatically via the following command:\n\n```shell\n$ poetry install\n```\n\nAlternatively, you can install dependencies with `pip`:\n\n```shell\n$ pip install -r requirements.txt\n```\n\n**Note**: Your python version must be `3.7.*` in order to install certain dependencies in this repository. \n\n**ii.** In this repository, we use `R` and `ggplot` for visualization. Execute the following within your R console to get the dependencies:\n\n```r\n> install.packages(c(\"ggplot2\",\"tikzDevice\",\"reshape2\",\"optparse\"))\n```\n\n**Note:** R-scripts were tested with R version `3.6.*`.\n\n### 2. Repository initialization\n\nIn order to populate this repository with the necessary data, follow these steps:\n\n**i.** Clone the `MIMIC-III` benchmarks [repository](https://github.com/YerevaNN/mimic3-benchmarks) and initialize input data for the In-Hospital-Mortality (IHM) task. This process can take a long period of time and also requires the user to undertake a medical ethics examination in order to download the `MIMIC-III` raw data.\n\n**ii.** After processing the IHM data, create a symlink from the `MIMIC-III` benchmark IHM data to `./data` in order to access the data in this repository:\n\n```shell\n$ cd ./data && ln -s {path-to-mimic3-repo}/data/in-hospital-mortality/ .\n```\n\n**iii.** Next, run `init.sh` in order to set up some final features:\n\n```shell\n$ ./init.sh\n```\n\n**iv**. The first prompt will ask if you want to initialize a pre-commit hook for keeping python-dependencies up-to-date. If you aim to further develop this repository, then answer yes for this prompt.\n\n**v.** The next prompt will ask if you want to download and deploy the best `MIMIC-III` benchmark models. Answer yes for this, as this step is necessary.\n\n### 3. Occlusion pipeline\n\nIn order to conduct occlusion on the `MIMIC-III` benchmark IHM dataset, we developed functions within `occlude.py`. We provide code for four types of occlusion; namely zero, normal-value, inner and outer occlusion. Details regarding each of these occlusions can be found in the py-docstring documentation in `occlude.py`. Below is a usage script:\n\n```\n$ python3 occlude.py --help\n\nusage: occlude.py [-h] [--occlusion-type str] [--n-iterations int]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --occlusion-type str  type of occlusion to conduct, either 'zero', 'normal-\n                        value', 'inner', 'outer' or 'all' (default: all)\n\narguments specific to inner/outer occlusion:\n  --n-iterations int    number of permutation iterations for inner/outer\n                        occlusion (default: 5)\n```\n\nAn example of running this script would be:\n\n```shell\n$ python3 occlude.py --occlusion-type all --n-iterations 10\n```\n\nThis pipeline will compute original and occluded model scores and will output them as `csv` files within the `./output` directory.\n\n### 4. Visualization\n\nIn order to visualize the results from the `csv` files, simply run the following:\n\n```shell\n$ Rscript vis.R\n```\n\nThe corresponding visualizations will be converted to latex code within the `tikz` framework and will then be saved as pdf's in the `./img` directory.\n\n**Note:** This requires all the corresponding `csv` files to be present.\n\n### 5. Interpretation\n\nBased on the results of the perturbation, we can already interpret some interesting results. Below, we have one of the visualizations produced from the step before.\n\n<p align=\"center\">\n<img src=\"./img/nom_violin.png\" width=\"1000\">\n</p>\n\nThis visualization shows nominal perturbations recorded during occlusion. `normal-value` on the top-right refers to replacing features of interest marginally by their `normal-value` or healthy values, which were suggested by medical experts. We shall focus on `normal-value` occlusion for this interpretation.\n\nMost features show non-clustered and generally widely distributed perturbations. The exception are the `presence*` features, which are binary features which indicate `1` when the feature was present in the timestep and `0` if the feature was absent during the timestep and needed to be imputed.\n\nFor the `presence*` features, we assumed the `normal` values would be `1`, since in a normal scenario, all features should be recorded for all timesteps. By doing this, we can see significant and clustered perturbations for the `presence*` features.\n\nIn particular, we can see that if (by chance) more `capillary_refill_rate` measurements were taken (such that `capillary_refill_rate` did not need to be imputed), the model's prediction probability for mortality decreases significantly. This could pose a risk of false negatives for mortality.\n\n### 6. Reference\n\nHarutyunyan et al. 2019\n\n```\n@article{Harutyunyan2019,\n  author={Harutyunyan, Hrayr and Khachatrian, Hrant and Kale, David C. and Ver Steeg,\n          Greg and Galstyan, Aram},\n  title={Multitask learning and benchmarking with clinical time series data},\n  journal={Scientific Data},\n  year={2019},\n  volume={6},\n  number={1},\n  pages={96},\n  issn={2052-4463},\n  doi={10.1038/s41597-019-0103-9},\n  url={https://doi.org/10.1038/s41597-019-0103-9}\n}\n```\n"}
{"url": "https://github.com/atreyasha/monix", "owner": "atreyasha", "repository_name": "monix", "date_all_variable_collection": "2023-07-04", "description": "Minimal Arch Linux build", "size": 251, "stargazers_count": 0, "watchers_count": 0, "language": "Shell", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "atreyasha", "contributions": 206}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Shell", "num_chars": 8242}, {"language": "Makefile", "num_chars": 4224}], "readme": "# monix\n\nThis repository documents workflows to *reproduce* a minimal Arch Linux build called `monix`. While this repository sets up root-level configurations for the OS, the [`dotfiles`](https://github.com/atreyasha/dotfiles) repository is used to set up user-level configurations. These are kept separate for modularity, but are designed to be used together. All setup workflows have been tested on both virtual machines and hardware.\n\n<p align=\"center\">\n<img src=\"https://archlinux.org/static/logos/legacy/arch-legacy-noodle-box.eb6d7aaefe13.svg\">\n</p>\n\nFor automatically formatting the development log in this repository, initialize a local pre-commit hook:\n\n```\n$ make init\n```\n\n**Note:** This repository focuses on approximate reproducibility oriented towards functionality. This implies that final builds may not be bit-for-bit identical.\n\n## Installation\n\n<details><summary>Live medium</summary><p>\n\nFor this first step, follow the instructions from the Arch Linux installation [guide](https://wiki.archlinux.org/title/Installation_guide).\n\n**Important:** During the `pacstrap` phase where basic packages are installed before `chroot`, use the command below instead. This ensures a text editor and an active internet connection will be available after `chroot` and the first reboot.\n\n```\n# pacstrap /mnt base base-devel linux-lts linux-firmware vim git networkmanager\n```\n\n**Note:** `nvidia` may not boot for Linux kernels 5.18 or greater, see issue [here](https://bbs.archlinux.org/viewtopic.php?id=283783) and [here](https://wiki.archlinux.org/title/NVIDIA). In this case, set the `ibt=off` kernel parameter in the boot loader.\n\n</p></details>\n<details><summary>Post reboot</summary>\n\n1. Log in to the freshly installed Arch Linux system as `root`\n\n2. Execute `EDITOR=vim visudo` to edit the `/etc/sudoers` file. Uncomment the following line to allow users from the `wheel` group to execute root-level commands with `sudo`:\n\n    ```\n    %wheel ALL=(ALL:ALL) ALL\n    ```\n\n3. Create a new user and add this user to the `wheel` group:\n\n    ```\n    # useradd -m -G wheel <username>\n    ```\n\n4. Set a password for this user:\n\n    ```\n    # passwd <username>\n    ```\n\n5. Execute `logout` and log in using your new user's details\n\n6. Start `NetworkManager.service` to ease the internet connection setup:\n\n    ```\n    $ sudo systemctl start NetworkManager.service\n    ```\n\n7. Configure your internet connection using `nmtui`, which should work for most connection types. Verify that your internet connection works by executing `ping www.example.com` and checking for successful packet transmission and receipt.\n\n    **Note:** `nmtui` could require `root` permissions at this point in time\n\n8. Clone this repository in your `$HOME` directory and install:\n\n    ```\n    $ git -C $HOME clone https://github.com/atreyasha/monix.git\n    $ cd $HOME/monix\n    $ make init\n    $ make install\n    ```\n\n9. Clone the [`dotfiles`](https://github.com/atreyasha/dotfiles) repository in your `$HOME` directory and install:\n\n    ```\n    $ git -C $HOME clone https://github.com/atreyasha/dotfiles.git\n    $ cd $HOME/dotfiles\n    $ make init\n    $ make install\n    ```\n\n10. If you have any private dotfiles and data, deploy them now. In order to benefit from all the features of the `dotfiles` repository, rename your private dotfiles repository as `privates` and place it in your `$HOME` directory. This `privates` repository should contain a `Makefile` with a `test` target\n\n11. Reboot and enjoy!\n\n</p></details>\n\n## Test\n\n[`bats-core`](https://github.com/bats-core/bats-core) is a dependency for unit testing. To run tests, execute:\n\n```\n$ make test\n```\n\n## Development\n\nCheck out our development [log](./docs/develop.md) for details on upcoming developments.\n"}
{"url": "https://github.com/atreyasha/mv-temporal-rgan", "owner": "atreyasha", "repository_name": "mv-temporal-rgan", "date_all_variable_collection": "2023-07-04", "description": "Multivariate recurrent GANs aimed at generating biomedical time-series. Methodology involves drawing symmetries to adversarial image generation", "size": 212097, "stargazers_count": 23, "watchers_count": 23, "language": "Python", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 4, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 4, "open_issues": 0, "watchers": 23, "default_branch": "main", "contributors": [{"contributor": "atreyasha", "contributions": 341}, {"contributor": "dependabot[bot]", "contributions": 3}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Python", "num_chars": 208478}, {"language": "R", "num_chars": 3631}, {"language": "Shell", "num_chars": 3495}], "readme": "## Multivariate recurrent GANs \n\n### Overview\n\nThis project is focused on developing recurrent GAN architecture(s) that can generate biomedical time series. In terms of methodologies, we are inspired by the RGAN and RCGAN (conditional RGAN) architecture proposed by [Esteban, Hyland and R\u00e4tsch 2017](https://arxiv.org/abs/1706.02633). In terms of biomedical data, we aim to work with the existing MIMIC-III benchmarks documented in [Harutyunyan, Khachatrian, Kale, Ver Steeg and Galstyan 2019](https://arxiv.org/abs/1703.07771) with the following public GitHub [repository](https://github.com/YerevaNN/mimic3-benchmarks).\n\n**Note:** This project is incomplete. Details of its current status can be found [here](https://github.com/atreyasha/mv-temporal-rgan/blob/master/src/README.md#caveats).\n\n### Dependencies\n\nThis repository's source code was tested with Python versions `3.7.*` and R versions `3.6.*`.\n\n1. Install python dependencies located in `requirements.txt`:\n\n    ```shell\n    $ pip install -r requirements.txt\n    ```\n\n2. Install R-based dependencies:\n\n    ```R\n    > install.packages(c(\"ggplot2\",\"tools\",\"extrafont\",\"reshape2\",\"optparse\",\"plyr\"))\n    ```\n\n3. Install [binary](https://github.com/nwtgck/gif-progress) for adding progress bar to produced gif's.\n\n4. **Optional:** To develop this repository, it is recommended to initialize a pre-commit hook for automatic updates of python dependencies:\n\n    ```shell\n    $ ./init.sh\n    ```\n\n### Workflow\n\nOur workflow and source code can be found in the `src` directory of this repository. Additionally, the [readme](/src/README.md) in the `src` directory documents our functions, scripts and results.\n\nA thorough development log for our ideas/progress can be found [here](/docs/todos.md).\n\n### References\n\nRelevant BibTeX entries for the above-mentioned papers can be found below:\n\nHarutyunyan et al. 2019 \n\n```\n@article{Harutyunyan_2019,\n   title={Multitask learning and benchmarking with clinical time series data},\n   volume={6},\n   ISSN={2052-4463},\n   url={http://dx.doi.org/10.1038/s41597-019-0103-9},\n   DOI={10.1038/s41597-019-0103-9},\n   number={1},\n   journal={Scientific Data},\n   publisher={Springer Science and Business Media LLC},\n   author={Harutyunyan, Hrayr and Khachatrian, Hrant and Kale, David C. \n   and Ver Steeg, Greg and Galstyan, Aram},\n   year={2019},\n   month={Jun}\n}\n```\n\nEsteban et al. 2017\n\n```\n@misc{esteban2017realvalued,\n    title={Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs},\n    author={Crist\u00f3bal Esteban and Stephanie L. Hyland and Gunnar R\u00e4tsch},\n    year={2017},\n    eprint={1706.02633},\n    archivePrefix={arXiv},\n    primaryClass={stat.ML}\n}\n```\n"}
{"url": "https://github.com/atreyasha/semantic-isometry-nmt", "owner": "atreyasha", "repository_name": "semantic-isometry-nmt", "date_all_variable_collection": "2023-07-04", "description": "Investigating the isometric behaviour of Neural Machine Translation models on binary semantic equivalence spaces", "size": 62492, "stargazers_count": 2, "watchers_count": 2, "language": "Python", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 2, "default_branch": "main", "contributors": [{"contributor": "atreyasha", "contributions": 492}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Python", "num_chars": 37804}, {"language": "Shell", "num_chars": 27950}, {"language": "R", "num_chars": 26126}], "readme": "## Investigating the isometric behaviour of Neural Machine Translation models on binary semantic equivalence spaces\n\n1. [Overview](#Overview-book)\n2. [Dependencies](#Dependencies-neckbeard)\n3. [Repository initialization](#Repository-initialization-fire)\n4. [Usage](#Usage-cyclone)\n    1. [Training](#i-Training)\n    2. [Translation](#ii-Translation)\n    3. [Evaluation](#iii-Evaluation)\n    4. [Visualization](#iv-Visualization)\n5. [Development](#Development-snail)\n\n### Overview :book:\n\nIsometry is defined mathematically as a distance-preserving transformation between two metric spaces. A simplified [illustration](https://slideplayer.com/slide/4659099/) of isometry in higher dimensional functional spaces can be seen below. In this research, we view Neural Machine Translation (NMT) models from the perspective of semantic isometry and assume that well-performing NMT models function approximately isometrically on semantic metric spaces. That is to say, if two sentences are semantically equivalent on the source side, they should remain semantically equivalent after translation on the target side given a well-performing NMT model. We hypothesize that the frequency of such semantically isometric behaviour correlates positively with general model performance. \n\n<p align=\"center\">\n<img src=\"./img/isometry_visualized_modified.png\" width=\"400\">\n</p>\n\nWe conduct our investigation by using two NMT models of varying performance to translate semantically-equivalent German paraphrases, based off diverse WMT19 test data [references](https://github.com/google/wmt19-paraphrased-references), to English. We use Facebook's AI Research's (FAIR) WMT19 winning single model from [Ng. et al. (2019)](https://arxiv.org/abs/1907.06616) as our SOTA model and abbreviate this as FAIR-WMT19. We train a large Transformer model based on the Scaling NMT methodology from [Ott et al. (2018)](https://arxiv.org/abs/1806.00187) on WMT16 data and utilize this model as our non-SOTA model. We abbreviate this model as STANDARD-WMT16.\n\nWe simplify the notion of semantic metric spaces into probabilistic binary semantic equivalence spaces and compute these using three Transformer language models fine-tuned on Google's [PAWS-X](https://github.com/google-research-datasets/paws/tree/master/pawsx) paraphrase detection task. We adapt our workflow from Google's [XTREME](https://github.com/google-research/xtreme) benchmark system.\n\nBy analyzing the paraphrase detection outputs, we show that the frequency of semantically isometric behaviour indeed correlates positively with general model performance. Our findings have interesting implications for automatic sequence evaluation metrics and vulnerabilities of NMT models towards adversarial paraphrases.\n\nA more detailed description of our methodologies and results can be found in this [paper](./docs/final_report/main.pdf).\n\n**Note:** `fairseq` and `XTREME` are used as third-party extensions in this repository with licensing details found [here](./THIRD_PARTY_NOTICES.txt).\n\n### Dependencies :neckbeard:\n\n1. This repository's code was tested with Python versions `3.7.*`. To sync dependencies, we recommend creating a virtual environment and installing the relevant packages via `pip`:\n\n    ```shell\n    pip install -r requirements.txt\n    ```\n\n2. In this repository, we use `R` versions `3.6.*` and `lualatex` for efficient `TikZ` visualizations. Execute the following within your `R` console to get the dependencies:\n\n    ```r\n    install.packages(c(\"ggplot2\",\"optparse\",\"tikzDevice\",\"rjson\",\"ggpointdensity\",\n                       \"fields\",\"gridExtra\",\"devtools\",\"reshape2\"))\n    devtools::install_github(\"teunbrand/ggh4x\")\n    ```\n\n### Repository initialization :fire:\n\n1. Initialize the [xtreme-pawsx](https://github.com/atreyasha/xtreme-pawsx) git submodule by running the following command:\n\n    ```shell\n    bash scripts/setup_xtreme_pawsx.sh\n    ```\n\n2. Manually download [preprocessed WMT'16 En-De data](https://drive.google.com/uc?export=download&id=0B_bZck-ksdkpM25jRUN2X2UxMm8) provided by Google and place the tarball in the `data` directory (~480 MB download size).\n\n3. Manually download the following four pre-trained models and place all of the tarballs in the `models` directory (~9 GB total download size):\n    1. [STANDARD-WMT16](https://drive.google.com/uc?id=1IqoRGIqTv9MVbV7EiyAZ7Ol0y4xAy0th&export=download) for non-SOTA `de-en` translation. Model achieved `BLEU-4` score of `31.0` on the `newstest2014` test data set.\n\n    2. [mBERT<sub>Base</sub>](https://drive.google.com/uc?id=18thE9bc-SVfwpAoeuOiZequp5YgckCA5&export=download) for multilingual paraphrase detection. Model fine-tuned on `en,de,es,fr,ja,ko,zh` languages with macro-F<sub>1</sub> score of `0.886`.\n    3. [XLM-R<sub>Base</sub>](https://drive.google.com/uc?id=1jSeopFJwKly7uk57mxLpQ9I5C5Fl3P1v&export=download) for multilingual paraphrase detection. Model fine-tuned on `en,de,es,fr,ja,ko,zh` languages with macro-F<sub>1</sub> score of `0.890`. \n    4. [XLM-R<sub>Large</sub>](https://drive.google.com/uc?id=1h6DUEpm173w_4aznMOdRFGtr6hi1bfdT&export=download) for multilingual paraphrase detection. Model fine-tuned on `en,de,es,fr,ja,ko,zh` languages with macro-F<sub>1</sub> score of `0.906`.\n\n3. Download `PAWS-X`, `WMT19` Legacy German paraphrases and `WMT19` AR German paraphrases, as well as prepare the previously downloaded `WMT16` data and pre-trained models by running the command below:\n\n    ```shell\n    bash scripts/prepare_data_models.sh\n    ```\n\n4. **Optional:** We provide a secondary branch `slurm-s3it` for executing computationally heavy workflows (eg. training, evaluating) on the `s3it` server with `slurm`. To use this branch, simply execute:\n\n    ```\n    git checkout slurm-s3it\n    ```\n\n5. **Optional:** If you want to further develop this repository; you can auto-format shell/R scripts and synchronize python dependencies, the development log and the `slurm-s3it` branch by initializing our pre-commit and pre-push `git` hooks:\n\n    ```shell\n    bash scripts/setup_git_hooks.sh\n    ```\n\n### Usage :cyclone: \n\n#### i. Training\n\nSince we already provide pre-trained models in this repository, we treat model training as an auxiliary procedure. If you would like to indeed train the non-SOTA STANDARD-WMT16 model and fine-tune paraphrase detection models, refer to the instructions in [TRAINING.md](TRAINING.md).\n\n#### ii. Translation\n\nIn order to translate WMT19 Legacy and WMT19 AR German paraphrases to English, utilize our script `translate_wmt19_paraphrases_de_en.sh`: \n\n```\nUsage: translate_wmt19_paraphrases_de_en.sh [-h|--help] [glob]\nTranslate WMT19 paraphrases using both torch-hub and local models\n\nOptional arguments:\n  -h, --help   Show this help message and exit\n  glob <glob>  Glob for finding local NMT model checkpoints, defaults to\n               \"./models/transformer_vaswani_wmt_en_de_big.wmt16.de-en.1594228573/\n               checkpoint_best.pt\"\n```\n\nThis script will generate translations using the SOTA FAIR-WMT19 and the non-SOTA STANDARD-WMT16 models. Translation results will be saved as `json` files in the `predictions` directory. To run this script using our defaults, simply execute:\n\n\n```shell\nbash scripts/translate_wmt19_paraphrases_de_en.sh \n```\n\n#### iii. Evaluation\n\n##### Commutative BLEU-4 and chrF-2\n\nAfter translating the WMT19 Legacy and WMT19 AR paraphrases, we can conduct a *quick and dirty* evaluation of source and target sentences using commutative variants of the `BLEU-4` and `chrF-2` automatic sequence evaluation metrics, which were initialized with the default settings from `sacrebleu`. For this, we provide `evaluate_bleu_chrf_wmt19_paraphrases_de_en.sh`:\n\n```\nUsage: evaluate_bleu_chrf_wmt19_paraphrases_de_en.sh [-h|--help] [glob]\nConduct shallow evaluation of WMT19 paraphrases with commutative\nBLEU-4 and chrF-2 scores\n\nOptional arguments:\n  -h, --help   Show this help message and exit\n  glob <glob>  Glob for finding input json translations, defaults to\n               \"./predictions/*/*.json\"\n```\n\nThis script will analyze source and target sentences in the aforementioned `json` files and will append commutative `BLEU-4` and `chrF-2` scores in-place. To run this script, simply execute:\n\n```shell\nbash scripts/evaluate_bleu_chrf_wmt19_paraphrases_de_en.sh\n```\n\n##### Paraphrase detection\n\nNext, we can run our fine-tuned paraphrase detection models on our source and target sentences. For this, we provide `evaluate_paraphrase_detection_wmt19_paraphrases_de_en.sh`:\n\n```\nUsage: evaluate_paraphrase_detection_wmt19_paraphrases_de_en.sh [-h|--help] [glob]\nConduct evaluation of WMT19 paraphrases using pre-trained paraphrase\ndetection models\n\nOptional arguments:\n  -h, --help   Show this help message and exit\n  glob <glob>  Glob for finding input json translations, defaults to\n               \"./predictions/*/*.json\"\n```\n\nThis script will analyze source and target sentences in the aforementioned `json` files and will append the paraphrase detection models' `softmax` scores for the paraphrase (or positive) label in-place. To run this script, simply execute:\n\n```shell\nbash scripts/evaluate_paraphrase_detection_wmt19_paraphrases_de_en.sh\n```\n\n#### iv. Visualization\n\n##### Model evolutions\n\nIn order to plot the evolutions of model-related training parameters, we provide `visualize_model_evolutions.sh`:\n\n```\nUsage: visualize_model_evolutions.sh [-h|--help] [glob]\nVisualize model evolutions for translation and paraphrase detection models\n\nOptional arguments:\n  -h, --help   Show this help message and exit\n  glob <glob>  Glob for finding tensorboard log directories, which will\n               be converted to csv's and then plotted. Defaults to\n               \"./models/*/{train,train_inner,valid}\"\n```\n\nThis script will aggregate tensorboard event logs into `csv` files and produce tikz-based plots of model evolutions as `pdf` files in the `img` directory. To run this script, simply execute:\n\n```shell\nbash scripts/visualize_model_evolutions.sh\n```\n\n##### Commutative chrF-2\n\nIn order to visualize the previously processed commutative `chrF-2` scores, we provide `visualize_chrf_wmt19_paraphrases_de_en.sh`:\n\n```\nUsage: visualize_chrf_wmt19_paraphrases_de_en.sh [-h|--help] [glob]\nVisualize commutative chrF-2 scores of WMT19 paraphrase translations\n\nOptional arguments:\n  -h, --help   Show this help message and exit\n  glob <glob>  Glob for finding input json translations, defaults to\n               \"./predictions/*/*.json\"\n```\n\nThis script will produce a tikz-based plot of the commutative `chrF-2` scores and will save it as `pdf` file in the `img` directory. To run this script, simply execute:\n\n```shell\nbash scripts/visualize_chrf_wmt19_paraphrases_de_en.sh\n```\n\n##### Paraphrase detection\n\nIn order to visualize the previously processed paraphrase detection results, we provide `visualize_paraphrase_detection_wmt19_paraphrases_de_en.sh`:\n\n```\nUsage: visualize_paraphrase_detection_wmt19_paraphrases_de_en.sh [-h|--help] [glob]\nVisualize paraphrase detection predictions of WMT19 paraphrase translations\n\nOptional arguments:\n  -h, --help   Show this help message and exit\n  glob <glob>  Glob for finding input json translations, defaults to\n               \"./predictions/*/*.json\"\n```\n\nThis script will produce tikz-based plots of the respective paraphrase detection `softmax` scores and joint model decisions, and will save them as `pdf` files in the `img` directory. To run this script, simply execute:\n\n```shell\nbash scripts/visualize_paraphrase_detection_wmt19_paraphrases_de_en.sh\n```\n\n##### Correlation between commutative chrF-2 and paraphrase detection predictions\n\nIn order to visualize correlations between commutative `chrF-2` scores and paraphrase detection predictions, we provide `visualize_paraphrase_detection_wmt19_paraphrases_de_en.sh`:\n\n```\nUsage: visualize_chrf_paraphrase_detection_wmt19_paraphrases_de_en.sh [-h|--help] [glob]\nVisualize commutative chrF-2 and paraphrase detection predictions of WMT19 paraphrase translations\n\nOptional arguments:\n  -h, --help   Show this help message and exit\n  glob <glob>  Glob for finding input json translations, defaults to\n               \"./predictions/*/*.json\"\n```\n\nThis script will produce tikz-based plots of correlations between commutative `chrF-2` scores and paraphrase detection predictions and will save them as `pdf` files in the `img` directory. To run this script, simply execute:\n\n```shell\nbash scripts/visualize_chrf_paraphrase_detection_wmt19_paraphrases_de_en.sh\n```\n\n<!--  LocalWords:  behaviour Isometry isometry NMT img src png WMT Ng et al Ott\n -->\n<!--  LocalWords:  SOTA neckbeard submodule preprocessed De pre eg FAIR's BLEU\n -->\n<!--  LocalWords:  chrF evolutions tensorboard tikz bibtex xtreme pawsx\n -->\n"}
{"url": "https://github.com/atreyasha/sentiment-argument-mining", "owner": "atreyasha", "repository_name": "sentiment-argument-mining", "date_all_variable_collection": "2023-07-04", "description": "Sentiment analysis and argumentation mining in UNSC Speeches", "size": 130244, "stargazers_count": 1, "watchers_count": 1, "language": "Python", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 1, "default_branch": "main", "contributors": [{"contributor": "atreyasha", "contributions": 474}, {"contributor": "julesha", "contributions": 16}, {"contributor": "dependabot[bot]", "contributions": 1}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": true, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Python", "num_chars": 58074}, {"language": "R", "num_chars": 22499}, {"language": "Shell", "num_chars": 4525}], "readme": "# Sentiment Analysis and Argumentation Mining in UNSC Speeches\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4813013.svg)](https://doi.org/10.5281/zenodo.4813013)\n\n## Overview\n\nThis project entails sentiment analysis and argumentation mining into the recently published UN security council speeches (UNSC) corpus which is publicly accessible [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KGVSYH). The UNSC corpus contains ~65,000 UN security council speeches from ~5,000 security council meetings from years 1995-2017. Each meeting is split up into the various speeches given by member countries. Furthermore, speeches are annotated with dates, topics and overall meeting outcomes.\n\nThe UNSC corpus is, however, not annotated for argumentation structures and sentiment polarities. In this project, we attempt to produce automatic machine-driven sentiment and argumentation annotations for the UNSC corpus; which could aid future human-driven annotations.\n\nTo find out more about our methodologies, read the next parts of this readme. Additionally, a list of documents detailing our methodologies can be found below:\n\n* [Preliminary presentation](/docs/prelim_presentation/main.pdf)\n* [Progress-update presentation](/docs/progress_presentation/main.pdf)\n* [Final Report](/docs/final_report/main.pdf)\n\n## Dependencies\n\n1.  We developed this repository using Python versions `3.7.*`. To sync python-based dependencies, we recommend creating a virtual environment and running the following command:\n\n    ```shell\n    $ pip install -r requirements.txt\n    ```\n\n2. We use `R` versions `3.6.*` and `ggplot` for pretty visualizations. Execute the following within your R console to get our R-based dependencies:\n\n    ```r\n    > install.packages(c(\"ggplot2\",\"tikzDevice\",\"reshape2\",\"optparse\",\"ggsci\"))\n    ```\n\n## Repository Initialization\n\n### Data and hooks\n\nIn order to prepare the necessary data and git hooks, simply run `init.sh` and you will receive the following prompts:\n\n```shell\n$ ./init.sh\n```\n\n1. You will be prompted to download and deploy the UNSC corpus files. This will download and unzip the corresponding files, but can take quite some time due to large file sizes.\n\n2. You will be prompted to download and deploy the US Election Debate corpus which is publicly accesible [here](https://github.com/ElecDeb60To16/Dataset). This will download and unzip the corresponding files, and should be fairly quick.\n\n3. **Optional:** Finally, you will be prompted to initialize a pre-commit hook which keeps python dependencies up-to-date in `requirements.txt`, lints R/shell scripts and converts `org` files to the GitHub markdown format. This is only necessary if you are further developing this repository.\n\n### Pre-trained argumentation model\n\nIn this repository, we provide our best performing argumentation mining model `./model_logs/2020_03_17_09_17_44_MSL512_grid_train/model_1.h5` as a Git [LFS](https://git-lfs.github.com/) entry.\n\n1. If `git-lfs` was already installed on your system prior to the cloning of this repository, our best performing model should have also been cloned in the `./model_logs` directory.\n\n2. If you installed `git-lfs` on your system after cloning this repository, execute `git lfs pull` in the repository to pull the best performing model. In case of syncing problems, check out this GitHub [issue](https://github.com/git-lfs/git-lfs/issues/325) for suggested workarounds.\n\n## Sentiment Analysis\n\nUnder sentiment analysis, we tested two successful sentiment-analysis tools; specifically VADER and AFINN, on the UNSC corpus. For subjectivity analysis, we used TextBlob, a text processing framework for Python. Next, we evaluated the predicted results to check their quality.\n\nFor further details on sentiment analysis, check out our dedicated Jupyter  [notebook](./sentiment.ipynb).\n\nOur final product for sentiment analysis is a [json](./data/UNSC/sentiment_annotation.json) file which maps UNSC speech IDs to automatically produced sentiment and subjectivity scores.\n\n## Argumentation Mining\n\nUnder argumentation mining, we fine-tuned the ALBERT language encoder with custom decoders on a small annotated political argumentation corpus known as the US Election Debate corpus. Next, we applied the fine-tuned argumentation classifier on the UNSC corpus to predict and extract argumentation candidates. \n\nFor further details on argumentation mining, check out our dedicated [readme](./argumentation.md).\n\nOur final products for argumentation mining are twofold; firstly being the fine-tuned ALBERT language [model](./model_logs/2020_03_17_09_17_44_MSL512_grid_train/model_1.h5) and secondly a human-readable [json](./data/UNSC/pred/pred_clean_512.json) file mapping UNSC speech IDs to token-level argumentation labels. For the `json` file, we were only able to conduct argumentation mining for shorter UNSC speeches.\n\n## References\n\nSch\u00f6nfeld et al. 2019 (UNSC corpus)\n\n```\n@misc{schnfeld2019security,\n  title={The UN Security Council debates 1995-2017},\n  author={Mirco Sch\u00f6nfeld and Steffen Eckhard and Ronny Patz and Hilde van Meegdenburg},\n  year={2019},\n  eprint={1906.10969},\n  archivePrefix={arXiv},\n  primaryClass={cs.DL}\n}\n```\n\nHaddadan et al. 2019 (US Election Debate corpus)\n\n```\n@inproceedings{haddadan-etal-2019-yes,\n  title = \"Yes, we can! Mining Arguments in 50 Years of {US} Presidential Campaign Debates\",\n  author = \"Haddadan, Shohreh  and\n    Cabrio, Elena  and\n    Villata, Serena\",\n  booktitle = {Proceedings of the 57th Annual Meeting of the Association\n  for Computational Linguistics},\n  month = jul,\n  year = \"2019\",\n  address = \"Florence, Italy\",\n  publisher = \"Association for Computational Linguistics\",\n  url = \"https://www.aclweb.org/anthology/P19-1463\",\n  doi = \"10.18653/v1/P19-1463\",\n  pages = \"4684--4690\"\n}\n```\n\n## Authors\n\nAtreya Shankar, Juliane Hanel\n\nProject Module: Mining Sentiments and Arguments, WiSe 2019/20\n\nCognitive Systems: Language, Learning, and Reasoning, University of Potsdam\n"}
{"url": "https://github.com/atreyasha/spacemacs-elpy", "owner": "atreyasha", "repository_name": "spacemacs-elpy", "date_all_variable_collection": "2023-07-04", "description": "Spacemacs layer(s) for Elpy, intended as a replacement for the Python layer", "size": 38, "stargazers_count": 0, "watchers_count": 0, "language": "Emacs Lisp", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "GNU General Public License v3.0", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "atreyasha", "contributions": 35}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Emacs Lisp", "num_chars": 8744}, {"language": "Makefile", "num_chars": 299}], "readme": "# spacemacs-elpy :heart:\n\nThis repository documents [Elpy](https://github.com/jorgenschaefer/elpy) as a Spacemacs layer, which is intended as a replacement for the Spacemacs `python` layer.\n\n## Usage\n\nHere we provide two implementations of Elpy, specifically a basic implementation which uses all upstream Elpy defaults (named `elpy`) and another which contains opinionated keybindings, toggles and auto-completion settings (named `elpy-plus`).\n\nFor detailed information, see the corresponding readmes under [elpy](/elpy) and [elpy-plus](/elpy-plus).\n\n### Installation\n\nWe assume all personal configuration code is housed in `~/.emacs.d`. Copy the relevant configuration code to `~/.emacs.d` by executing:\n\n```shell\n$ make install\n```\n\nTo trigger a complete installation, append either `elpy` or `elpy-plus` to your `.spacemacs` dotfile under `dotspacemacs-configuration-layers` and reload `emacs`.\n\n### Uninstallation\n\nFor a complete uninstallation, remove `elpy` or `elpy-plus` from your `.spacemacs` dotfile. Next, execute the following to remove the aforementioned configuration code:\n\n```shell\n$ make uninstall\n```\n\n## Bugs/Issues\n\nFeel free to open a GitHub issue in case of any bugs.\n\n<!--  LocalWords:  Spacemacs Elpy readmes elpy dotfile Uninstallation\n -->\n<!--  LocalWords:  uninstallation autocompletion autocompletions\n -->\n"}
{"url": "https://github.com/atreyasha/spam-detection", "owner": "atreyasha", "repository_name": "spam-detection", "date_all_variable_collection": "2023-07-04", "description": "Investigating whether sequential learning is inherently important for the task of spam classification, through a comparison of SVM vs. CNN-LSTM models", "size": 53992, "stargazers_count": 4, "watchers_count": 4, "language": "Python", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 4, "default_branch": "main", "contributors": [{"contributor": "atreyasha", "contributions": 115}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Python", "num_chars": 61019}, {"language": "R", "num_chars": 8439}, {"language": "Shell", "num_chars": 4166}], "readme": "## Comparison of SVM (non-sequential) vs. CNN-LSTM (sequential) models for supervised spam classification\n\nThe central objective of this repository is to investigate whether sequential learning is inherently important for the task of supervised spam classification. This is done by implementing a sequential CNN-LSTM model and comparing it with a non-sequential SVM classifier. Aspects such as generalization accuracy and robustness of these models will be explored in this repository.\n\nThe models will be trained on the pre-processed `enron-spam` [dataset](http://www2.aueb.gr/users/ion/data/enron-spam/), consisting of ~34k instances of roughly balanced \"ham\" and spam emails.\n\nTo initialize this repository, it is recommended to enable a pre-commit hook which updates python dependencies in `requirements.txt`.\n\n```shell\n$ ./init.sh\n```\n\n## Dependencies\n\nThis repository's source code was tested with Python versions `3.7.*` and R versions `3.6.*`.\n\n1. Install python dependencies located in `requirements.txt`:\n\n    ```shell\n    $ pip install -r requirements.txt\n    ```\n\n2. Install R-based dependencies:\n\n    ```R\n    > install.packages(c(\"RcppCNPy\",\"data.table\",\"ggplot2\",\"latex2exp\",\n                         \"extrafont\",\"ggsci\",\"optparse\"))\n    ```\n\n## Workflow\n\nFurther information on data pre-processing, the models and results/evaluations can be found in the `/src` directory and the corresponding [readme](/src/README.md).\n\n## Reference\n\nBelow is the BibTeX entry for Metsis et al. 2006, which is the paper published alongside the pre-processed enron dataset in this repository.\n\n```\n@inproceedings{metsis2006spam,\n  title={Spam filtering with naive bayes-which naive bayes?},\n  author={Metsis, Vangelis and Androutsopoulos, Ion and Paliouras, Georgios},\n  booktitle={CEAS},\n  volume={17},\n  pages={28--69},\n  year={2006},\n  organization={Mountain View, CA}\n}\n```\n"}
{"url": "https://github.com/atreyasha/spp-explainability", "owner": "atreyasha", "repository_name": "spp-explainability", "date_all_variable_collection": "2023-07-04", "description": "SoPa++: Leveraging explainability from hybridized RNN, CNN and weighted finite-state neural architectures", "size": 40711, "stargazers_count": 3, "watchers_count": 3, "language": "Python", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 3, "default_branch": "main", "contributors": [{"contributor": "atreyasha", "contributions": 863}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Python", "num_chars": 171578}, {"language": "Shell", "num_chars": 30103}, {"language": "R", "num_chars": 14600}], "readme": "# SoPa++\n\nThis repository documents M.Sc. thesis research titled *\"SoPa++: Leveraging explainability from hybridized RNN, CNN and weighted finite-state neural architectures\"*. This research was adapted from the original [SoPa](https://github.com/Noahs-ARK/soft_patterns) model in Schwartz, Thomson and Smith (2018), which is distributed under the MIT [License](./THIRD_PARTY_NOTICES.txt).\n\nFor more details, check out the following documents:\n\n* [Manuscript](./docs/manuscript/main.pdf)\n* [Defense](./docs/defense/main.pdf)\n\n## Dependencies :neckbeard:\n\n1. This repository's code was tested with Python versions `3.7.*`. To sync dependencies, we recommend creating a virtual environment and installing the relevant packages via `pip`:\n\n    ```shell\n    pip install -r requirements.txt\n    ```\n\n    **Note:** If you intend to use the GPU, the `torch==1.7.0` dependency in `requirements.txt` works out-of-the-box with CUDA version `10.2`. If you have a different version of CUDA, refer to the official [PyTorch](https://pytorch.org/get-started/locally/) webpage for alternative `pip` installation commands which will provide `torch` optimized for your CUDA version.\n\n2. We use `R` for visualizations integrated with `TikZ` and `ggplot`. Below is the `sessionInfo()` output, which can be used for replicating our dependencies explicitly.\n\n    ```\n    R version 4.0.4 (2021-02-15)\n    Platform: x86_64-pc-linux-gnu (64-bit)\n    Running under: Arch Linux\n\n    Matrix products: default\n    BLAS:   /usr/lib/libblas.so.3.9.0\n    LAPACK: /usr/lib/liblapack.so.3.9.0\n\n    locale:\n     [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n     [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n     [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n     [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n     [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n    [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\n    attached base packages:\n    [1] tools     stats     graphics  grDevices utils     datasets  methods  \n    [8] base     \n\n    other attached packages:\n    [1] RColorBrewer_1.1-2  plyr_1.8.6          reshape2_1.4.4     \n    [4] optparse_1.6.6      tikzDevice_0.12.3.1 rjson_0.2.20       \n    [7] ggh4x_0.1.2.1       ggplot2_3.3.3      \n    ```\n\n## Repository initialization :fire:\n\n1. Automatically download and prepare [GloVe-6B](https://nlp.stanford.edu/projects/glove/) word embeddings and the Facebook Multilingual Task Oriented Dialogue (FMTOD) [data set](https://research.fb.com/publications/cross-lingual-transfer-learning-for-multilingual-task-oriented-dialog/):\n\n    ```shell\n    bash scripts/prepare_data.sh\n    ```\n\n2. **Optional:** Manually download our [pre-trained models](https://drive.google.com/file/d/1Q2cqL08_D8-UvZk8FXZMs59f6b7PUaI2/view?usp=sharing) and place the `models.tar.gz` tarball in the `models` directory (~5 GB download size). Next, execute the following to prepare all models:\n\n    ```shell\n    bash scripts/prepare_models.sh\n    ```\n\n3. **Optional:** Initialize git hooks to manage development workflows such as linting shell/R scripts, keeping python dependencies up-to-date and formatting the development log:\n\n    ```shell\n    bash scripts/prepare_git_hooks.sh\n    ```\n\n## Usage :snowflake:\n\n### SoPa++\n\n<details><summary>i. Preprocessing</summary>\n<p>\n\nFor preprocessing the FMTOD data set, we use `src/preprocess_fmtod.py`:\n\n```\nusage: preprocess_fmtod.py [-h] [--data-directory <dir_path>]\n                           [--disable-upsampling]\n                           [--logging-level {debug,info,warning,error,critical}]\n                           [--truecase]\n\noptional arguments:\n  -h, --help            show this help message and exit\n\noptional preprocessing arguments:\n  --data-directory      <dir_path>\n                        Data directory containing clean input data (default:\n                        ./data/fmtod/)\n  --disable-upsampling  Disable upsampling on the train and validation data\n                        sets (default: False)\n  --truecase            Retain true casing when preprocessing data. Otherwise\n                        data will be lowercased by default (default: False)\n\noptional logging arguments:\n  --logging-level       {debug,info,warning,error,critical}\n                        Set logging level (default: info)\n```\n\nThe default workflow cleans the original FMTOD data, forces it to lowercased format and upsamples all minority classes. To run the default workflow, execute:\n\n```shell\nbash scripts/preprocess_fmtod.sh\n```\n\n</p>\n</details>\n\n<details><summary>ii. Training</summary>\n<p>\n\nFor training the SoPa++ model, we use `src/train_spp.py`:\n\n```\nusage: train_spp.py [-h] --embeddings <file_path> --train-data <file_path>\n                    --train-labels <file_path> --valid-data <file_path>\n                    --valid-labels <file_path> [--batch-size <int>]\n                    [--bias-scale <float>] [--clip-threshold <float>]\n                    [--disable-scheduler] [--disable-tqdm] [--dropout <float>]\n                    [--epochs <int>] [--evaluation-period <int>] [--gpu]\n                    [--gpu-device <str>] [--grid-config <file_path>]\n                    [--grid-training] [--learning-rate <float>]\n                    [--logging-level {debug,info,warning,error,critical}]\n                    [--max-doc-len <int>] [--max-train-instances <int>]\n                    [--models-directory <dir_path>] [--no-wildcards]\n                    [--num-random-iterations <int>] [--only-epoch-eval]\n                    [--patience <int>] [--patterns <str>]\n                    [--scheduler-factor <float>] [--scheduler-patience <int>]\n                    [--seed <int>]\n                    [--semiring {MaxSumSemiring,MaxProductSemiring}]\n                    [--static-embeddings] [--tau-threshold <float>]\n                    [--torch-num-threads <int>] [--tqdm-update-period <int>]\n                    [--wildcard-scale <float>] [--word-dropout <float>]\n\noptional arguments:\n  -h, --help               show this help message and exit\n\nrequired training arguments:\n  --embeddings             <file_path>\n                           Path to GloVe token embeddings file (default: None)\n  --train-data             <file_path>\n                           Path to train data file (default: None)\n  --train-labels           <file_path>\n                           Path to train labels file (default: None)\n  --valid-data             <file_path>\n                           Path to validation data file (default: None)\n  --valid-labels           <file_path>\n                           Path to validation labels file (default: None)\n\noptional training arguments:\n  --batch-size             <int>\n                           Batch size for training (default: 256)\n  --clip-threshold         <float>\n                           Gradient clipping threshold (default: None)\n  --disable-scheduler      Disable learning rate scheduler which reduces\n                           learning rate on performance plateau (default:\n                           False)\n  --dropout                <float>\n                           Neuron dropout probability (default: 0.2)\n  --epochs                 <int>\n                           Maximum number of training epochs (default: 50)\n  --evaluation-period      <int>\n                           Specify after how many training updates should\n                           model evaluation(s) be conducted. Evaluation will\n                           always be conducted at the end of epochs (default:\n                           100)\n  --learning-rate          <float>\n                           Learning rate for Adam optimizer (default: 0.001)\n  --max-doc-len            <int>\n                           Maximum document length allowed (default: None)\n  --max-train-instances    <int>\n                           Maximum number of training instances (default:\n                           None)\n  --models-directory       <dir_path>\n                           Base directory where all models will be saved\n                           (default: ./models)\n  --only-epoch-eval        Only evaluate model at the end of epoch, instead of\n                           evaluation by updates (default: False)\n  --patience               <int>\n                           Number of epochs with no improvement after which\n                           training will be stopped (default: 10)\n  --scheduler-factor       <float>\n                           Factor by which the learning rate will be reduced\n                           (default: 0.1)\n  --scheduler-patience     <int>\n                           Number of epochs with no improvement after which\n                           learning rate will be reduced (default: 5)\n  --seed                   <int>\n                           Global random seed for numpy and torch (default:\n                           42)\n  --word-dropout           <float>\n                           Word dropout probability (default: 0.2)\n\noptional grid-training arguments:\n  --grid-config            <file_path>\n                           Path to grid configuration file (default:\n                           ./src/resources/flat_grid_large_config.json)\n  --grid-training          Use grid-training instead of single-training\n                           (default: False)\n  --num-random-iterations  <int>\n                           Number of random iteration(s) for each grid\n                           instance (default: 10)\n\noptional spp-architecture arguments:\n  --bias-scale             <float>\n                           Scale biases by this parameter (default: 1.0)\n  --no-wildcards           Do not use wildcard transitions (default: False)\n  --patterns               <str>\n                           Pattern lengths and counts with the following\n                           syntax: PatternLength1-PatternCount1_PatternLength2\n                           -PatternCount2_... (default: 6-50_5-50_4-50_3-50)\n  --semiring               {MaxSumSemiring,MaxProductSemiring}\n                           Specify which semiring to use (default:\n                           MaxSumSemiring)\n  --static-embeddings      Freeze learning of token embeddings (default:\n                           False)\n  --tau-threshold          <float>\n                           Specify value of TauSTE binarizer tau threshold\n                           (default: 0.0)\n  --wildcard-scale         <float>\n                           Scale wildcard(s) by this parameter (default: None)\n\noptional hardware-acceleration arguments:\n  --gpu                    Use GPU hardware acceleration (default: False)\n  --gpu-device             <str>\n                           GPU device specification in case --gpu option is\n                           used (default: cuda:0)\n  --torch-num-threads      <int>\n                           Set the number of threads used for CPU intraop\n                           parallelism with PyTorch (default: None)\n\noptional logging arguments:\n  --logging-level          {debug,info,warning,error,critical}\n                           Set logging level (default: info)\n\noptional progress-bar arguments:\n  --disable-tqdm           Disable tqdm progress bars (default: False)\n  --tqdm-update-period     <int>\n                           Specify after how many training updates should the\n                           tqdm progress bar be updated with model diagnostics\n                           (default: 5)\n```\n\n#### SoPa++ model training\n\nTo train a single SoPa++ model using our defaults on the CPU, execute:\n\n```shell\nbash scripts/train_spp.sh\n```\n\nTo train a single SoPa++ model using our defaults on a single GPU, execute:\n\n```shell\nbash scripts/train_spp_gpu.sh\n```\n\n#### Grid-based SoPa++ model training\n\nTo apply grid-based training on SoPa++ models using our defaults on the CPU, execute:\n\n```shell\nbash scripts/train_spp_grid.sh\n```\n\nTo apply grid-based training on SoPa++ models using our defaults on a single GPU, execute:\n\n```shell\nbash scripts/train_spp_grid_gpu.sh\n```\n\n</p>\n</details>\n\n<details><summary>iii. Resume training</summary>\n<p>\n\nFor resuming the aforementioned training workflow in case of interruptions, we use `src/train_resume_spp.py`:\n\n```\nusage: train_resume_spp.py [-h] --model-log-directory <dir_path>\n                           [--disable-tqdm] [--gpu] [--gpu-device <str>]\n                           [--grid-training]\n                           [--logging-level {debug,info,warning,error,critical}]\n                           [--torch-num-threads <int>]\n                           [--tqdm-update-period <int>]\n\noptional arguments:\n  -h, --help             show this help message and exit\n\nrequired training arguments:\n  --model-log-directory  <dir_path>\n                         Base model directory containing model data to be\n                         resumed for training (default: None)\n\noptional grid-training arguments:\n  --grid-training        Use grid-training instead of single-training\n                         (default: False)\n\noptional hardware-acceleration arguments:\n  --gpu                  Use GPU hardware acceleration (default: False)\n  --gpu-device           <str>\n                         GPU device specification in case --gpu option is used\n                         (default: cuda:0)\n  --torch-num-threads    <int>\n                         Set the number of threads used for CPU intraop\n                         parallelism with PyTorch (default: None)\n\noptional logging arguments:\n  --logging-level        {debug,info,warning,error,critical}\n                         Set logging level (default: info)\n\noptional progress-bar arguments:\n  --disable-tqdm         Disable tqdm progress bars (default: False)\n  --tqdm-update-period   <int>\n                         Specify after how many training updates should the\n                         tqdm progress bar be updated with model diagnostics\n                         (default: 5)\n```\n\n#### Resume SoPa++ model training\n\nTo resume training of a single SoPa++ model using our defaults on the CPU, execute:\n\n```shell\nbash scripts/train_resume_spp.sh /path/to/model/log/directory\n```\n\nTo resume training of a single SoPa++ model using our defaults on a single GPU, execute:\n\n```shell\nbash scripts/train_resume_spp_gpu.sh /path/to/model/log/directory\n```\n\n#### Resume grid-based SoPa++ model training\n\nTo resume grid-based training of SoPa++ models using our defaults on the CPU, execute:\n\n```shell\nbash scripts/train_resume_spp_grid.sh /path/to/model/log/directory\n```\n\nTo resume grid-based training of SoPa++ models using our defaults on a single GPU, execute:\n\n```shell\nbash scripts/train_resume_spp_grid_gpu.sh /path/to/model/log/directory\n```\n\n</p>\n</details>\n\n<details><summary>iv. Evaluation</summary>\n<p>\n\nFor evaluating trained SoPa++ model(s), we use `src/evaluate_spp.py`:\n\n```\nusage: evaluate_spp.py [-h] --eval-data <file_path> --eval-labels <file_path>\n                       --model-checkpoint <glob_path> [--batch-size <int>]\n                       [--evaluation-metric {recall,precision,f1-score,accuracy}]\n                       [--evaluation-metric-type {weighted avg,macro avg}]\n                       [--gpu] [--gpu-device <str>] [--grid-evaluation]\n                       [--logging-level {debug,info,warning,error,critical}]\n                       [--max-doc-len <int>] [--output-prefix <str>]\n                       [--torch-num-threads <int>]\n\noptional arguments:\n  -h, --help                show this help message and exit\n\nrequired evaluation arguments:\n  --eval-data               <file_path>\n                            Path to evaluation data file (default: None)\n  --eval-labels             <file_path>\n                            Path to evaluation labels file (default: None)\n  --model-checkpoint        <glob_path>\n                            Glob path to model checkpoint(s) with '.pt'\n                            extension (default: None)\n\noptional evaluation arguments:\n  --batch-size              <int>\n                            Batch size for evaluation (default: 256)\n  --max-doc-len             <int>\n                            Maximum document length allowed (default: None)\n  --output-prefix           <str>\n                            Prefix for output classification report (default:\n                            test)\n\noptional grid-evaluation arguments:\n  --evaluation-metric       {recall,precision,f1-score,accuracy}\n                            Specify which evaluation metric to use for\n                            comparison (default: f1-score)\n  --evaluation-metric-type  {weighted avg,macro avg}\n                            Specify which type of evaluation metric to use\n                            (default: weighted avg)\n  --grid-evaluation         Use grid-evaluation framework to find/summarize\n                            best model (default: False)\n\noptional hardware-acceleration arguments:\n  --gpu                     Use GPU hardware acceleration (default: False)\n  --gpu-device              <str>\n                            GPU device specification in case --gpu option is\n                            used (default: cuda:0)\n  --torch-num-threads       <int>\n                            Set the number of threads used for CPU intraop\n                            parallelism with PyTorch (default: None)\n\noptional logging arguments:\n  --logging-level           {debug,info,warning,error,critical}\n                            Set logging level (default: info)\n```\n\n#### SoPa++ model evaluation\n\nTo evaluate SoPa++ model(s) using our defaults on the CPU, execute:\n\n```shell\nbash scripts/evaluate_spp.sh \"/glob/to/neural/model/*/checkpoint(s)\"\n```\n\nTo evaluate SoPa++ model(s) using our defaults on a single GPU, execute:\n\n```shell\nbash scripts/evaluate_spp_gpu.sh \"/glob/to/neural/model/*/checkpoint(s)\"\n```\n\n#### Grid-based SoPa++ model evaluation\n\nTo evaluate grid-based SoPa++ models using our defaults on the CPU, execute:\n\n```shell\nbash scripts/evaluate_spp_grid.sh \"/glob/to/neural/model/*/checkpoints\"\n```\n\nTo evaluate grid-based SoPa++ models using our defaults on a single GPU, execute:\n\n```shell\nbash scripts/evaluate_spp_grid_gpu.sh \"/glob/to/neural/model/*/checkpoints\"\n```\n\n</p>\n</details>\n\n### RE proxy\n\n<details><summary>i. Explanations by simplification</summary>\n<p>\n\nFor explaining SoPa++ model(s) by simplifying it into a RE proxy model, we use `src/explain_simplify_spp.py`:\n\n```\nusage: explain_simplify_spp.py [-h] --neural-model-checkpoint <glob_path>\n                               --train-data <file_path> --train-labels\n                               <file_path> --valid-data <file_path>\n                               --valid-labels <file_path> [--atol <float>]\n                               [--batch-size <int>] [--disable-tqdm] [--gpu]\n                               [--gpu-device <str>]\n                               [--logging-level {debug,info,warning,error,critical}]\n                               [--max-doc-len <int>]\n                               [--max-train-instances <int>]\n                               [--torch-num-threads <int>]\n                               [--tqdm-update-period <int>]\n\noptional arguments:\n  -h, --help                 show this help message and exit\n\nrequired explainability arguments:\n  --neural-model-checkpoint  <glob_path>\n                             Glob path to neural model checkpoint(s) with\n                             '.pt' extension (default: None)\n  --train-data               <file_path>\n                             Path to train data file (default: None)\n  --train-labels             <file_path>\n                             Path to train labels file (default: None)\n  --valid-data               <file_path>\n                             Path to validation data file (default: None)\n  --valid-labels             <file_path>\n                             Path to validation labels file (default: None)\n\noptional explainability arguments:\n  --atol                     <float>\n                             Specify absolute tolerance when comparing\n                             equivalences between tensors (default: 1e-06)\n  --batch-size               <int>\n                             Batch size for explainability (default: 256)\n  --max-doc-len              <int>\n                             Maximum document length allowed (default: None)\n  --max-train-instances      <int>\n                             Maximum number of training instances (default:\n                             None)\n\noptional hardware-acceleration arguments:\n  --gpu                      Use GPU hardware acceleration (default: False)\n  --gpu-device               <str>\n                             GPU device specification in case --gpu option is\n                             used (default: cuda:0)\n  --torch-num-threads        <int>\n                             Set the number of threads used for CPU intraop\n                             parallelism with PyTorch (default: None)\n\noptional logging arguments:\n  --logging-level            {debug,info,warning,error,critical}\n                             Set logging level (default: info)\n\noptional progress-bar arguments:\n  --disable-tqdm             Disable tqdm progress bars (default: False)\n  --tqdm-update-period       <int>\n                             Specify after how many training updates should\n                             the tqdm progress bar be updated with model\n                             diagnostics (default: 5)\n```\n\nTo simplify SoPa++ model(s) using our defaults on the CPU, execute:\n\n```shell\nbash scripts/explain_simplify_spp.sh \"/glob/to/neural/model/*/checkpoint(s)\"\n```\n\nTo simplify SoPa++ model(s) using our defaults on a GPU, execute:\n\n```shell\nbash scripts/explain_simplify_spp_gpu.sh \"/glob/to/neural/model/*/checkpoint(s)\"\n```\n\n</p>\n</details>\n\n<details><summary>ii. Compression</summary>\n<p>\n\nFor compressing RE proxy model(s), we use `src/explain_compress_regex.py`:\n\n```\nusage: explain_compress_regex.py [-h] --regex-model-checkpoint <glob_path>\n                                 [--disable-tqdm]\n                                 [--logging-level {debug,info,warning,error,critical}]\n                                 [--tqdm-update-period <int>]\n\noptional arguments:\n  -h, --help                show this help message and exit\n\nrequired explainability arguments:\n  --regex-model-checkpoint  <glob_path>\n                            Glob path to regex model checkpoint(s) with '.pt'\n                            extension (default: None)\n\noptional logging arguments:\n  --logging-level           {debug,info,warning,error,critical}\n                            Set logging level (default: info)\n\noptional progress-bar arguments:\n  --disable-tqdm            Disable tqdm progress bars (default: False)\n  --tqdm-update-period      <int>\n                            Specify after how many training updates should the\n                            tqdm progress bar be updated with model\n                            diagnostics (default: 5)\n```\n\nTo compress RE proxy model(s) using our defaults on the CPU, execute:\n\n```shell\nbash scripts/explain_compress_regex.sh \"/glob/to/regex/model/*/checkpoint(s)\"\n```\n\n</p>\n</details>\n\n<details><summary>iii. Evaluation</summary>\n<p>\n\nFor evaluating RE proxy model(s), we use `src/evaluate_regex.py`:\n\n```\nusage: evaluate_regex.py [-h] --eval-data <file_path> --eval-labels\n                         <file_path> --model-checkpoint <glob_path>\n                         [--batch-size <int>] [--disable-tqdm] [--gpu]\n                         [--gpu-device <str>]\n                         [--logging-level {debug,info,warning,error,critical}]\n                         [--max-doc-len <int>] [--output-prefix <str>]\n                         [--torch-num-threads <int>]\n                         [--tqdm-update-period <int>]\n\noptional arguments:\n  -h, --help            show this help message and exit\n\nrequired evaluation arguments:\n  --eval-data           <file_path>\n                        Path to evaluation data file (default: None)\n  --eval-labels         <file_path>\n                        Path to evaluation labels file (default: None)\n  --model-checkpoint    <glob_path>\n                        Glob path to model checkpoint(s) with '.pt' extension\n                        (default: None)\n\noptional evaluation arguments:\n  --batch-size          <int>\n                        Batch size for evaluation (default: 256)\n  --max-doc-len         <int>\n                        Maximum document length allowed (default: None)\n  --output-prefix       <str>\n                        Prefix for output classification report (default:\n                        test)\n\noptional hardware-acceleration arguments:\n  --gpu                 Use GPU hardware acceleration (default: False)\n  --gpu-device          <str>\n                        GPU device specification in case --gpu option is used\n                        (default: cuda:0)\n  --torch-num-threads   <int>\n                        Set the number of threads used for CPU intraop\n                        parallelism with PyTorch (default: None)\n\noptional logging arguments:\n  --logging-level       {debug,info,warning,error,critical}\n                        Set logging level (default: info)\n\noptional progress-bar arguments:\n  --disable-tqdm        Disable tqdm progress bars (default: False)\n  --tqdm-update-period  <int>\n                        Specify after how many training updates should the\n                        tqdm progress bar be updated with model diagnostics\n                        (default: 5)\n```\n\nTo evaluate RE proxy model(s) using our defaults on the CPU, execute:\n\n```shell\nbash scripts/evaluate_regex.sh \"/glob/to/regex/model/*/checkpoint(s)\"\n```\n\nTo evaluate RE proxy model(s) using our defaults on a single GPU, execute:\n\n```shell\nbash scripts/evaluate_regex_gpu.sh \"/glob/to/regex/model/*/checkpoint(s)\"\n```\n\n</p>\n</details>\n\n### Comparisons and visualizations\n\n<details><summary>i. Model pair comparison</summary>\n<p>\n\nFor comparing SoPa++ and RE proxy model pair(s), we use `src/compare_model_pairs.py`:\n\n```\nusage: compare_model_pairs.py [-h] --eval-data <file_path> --eval-labels\n                              <file_path> --model-log-directory <glob_path>\n                              [--atol <float>] [--batch-size <int>]\n                              [--disable-tqdm] [--gpu] [--gpu-device <str>]\n                              [--logging-level {debug,info,warning,error,critical}]\n                              [--max-doc-len <int>] [--output-prefix <str>]\n                              [--torch-num-threads <int>]\n                              [--tqdm-update-period <int>]\n\noptional arguments:\n  -h, --help             show this help message and exit\n\nrequired evaluation arguments:\n  --eval-data            <file_path>\n                         Path to evaluation data file (default: None)\n  --eval-labels          <file_path>\n                         Path to evaluation labels file (default: None)\n  --model-log-directory  <glob_path>\n                         Glob path to model log directory/directories which\n                         contain both the best neural and compressed regex\n                         models (default: None)\n\noptional evaluation arguments:\n  --atol                 <float>\n                         Specify absolute tolerance when comparing\n                         equivalences between tensors (default: 1e-06)\n  --batch-size           <int>\n                         Batch size for evaluation (default: 256)\n  --max-doc-len          <int>\n                         Maximum document length allowed (default: None)\n  --output-prefix        <str>\n                         Prefix for output classification report (default:\n                         test)\n\noptional hardware-acceleration arguments:\n  --gpu                  Use GPU hardware acceleration (default: False)\n  --gpu-device           <str>\n                         GPU device specification in case --gpu option is used\n                         (default: cuda:0)\n  --torch-num-threads    <int>\n                         Set the number of threads used for CPU intraop\n                         parallelism with PyTorch (default: None)\n\noptional logging arguments:\n  --logging-level        {debug,info,warning,error,critical}\n                         Set logging level (default: info)\n\noptional progress-bar arguments:\n  --disable-tqdm         Disable tqdm progress bars (default: False)\n  --tqdm-update-period   <int>\n                         Specify after how many training updates should the\n                         tqdm progress bar be updated with model diagnostics\n                         (default: 5)\n```\n\nTo compare SoPa++ and RE proxy model pair(s) using our defaults on the CPU, execute:\n\n```shell\nbash scripts/compare_model_pairs.sh \"/glob/to/model/log/*/director(ies)\"\n```\n\nTo compare SoPa++ and RE proxy model pair(s) using our defaults on a GPU, execute:\n\n```shell\nbash scripts/compare_model_pairs_gpu.sh \"/glob/to/model/log/*/director(ies)\"\n```\n\n</p>\n</details>\n\n<details><summary>ii. FMTOD summary statistics</summary>\n<p>\n\nFor visualizing the FMTOD data set summary statistics, we apply functions from `src/visualize_fmtod.R`. This workflow is wrapped using `scripts/visualize_fmtod.sh`:\n\n```\nUsage: visualize_fmtod.sh [-h|--help]\n\nVisualize FMTOD data set summary statistics\n\nOptional arguments:\n  -h, --help  Show this help message and exit\n```\n\nTo visualize the FMTOD data set summary statistics, simply execute:\n\n```shell\nbash scripts/visualize_fmtod.sh\n```\n\n</p>\n</details>\n\n<details><summary>iii. Grid-based training</summary>\n<p>\n\nFor visualizing grid-based training performance, we use `src/tensorboard_event2csv.py` to convert tensorboard event logs to `csv` files and apply functions from `src/visualize_grid.R` to plot them. These two scripts are bound together by `scripts/visualize_grid_train.sh`:\n\n```\nUsage: visualize_grid_train.sh [-h|--help] tb_event_directory\n\nVisualize grid training performance for SoPa++ models,\ngiven that grid allows for the following varying arguments:\npatterns, tau_threshold, seed\n\nOptional arguments:\n  -h, --help                      Show this help message and exit\n\nRequired arguments:\n  tb_event_directory <glob_path>  Tensorboard event log directory/\n                                  directories\n```\n\nTo produce a facet-based visualization of grid-based training, simply execute:\n\n```shell\nbash scripts/visualize_grid_train.sh \"/glob/to/tb/event/*/director(ies)\"\n```\n\n**Note:** This script has been hard-coded for grid-based training scenarios where only the following three training/model arguments are varied: `patterns`, `tau_threshold` and `seed`.\n\n</p>\n</details>\n\n<details><summary>iv. Grid-based evaluation</summary>\n<p>\n\nFor visualizing grid-based evaluation performance and model-pair distances, we apply functions from `src/visualize_grid.R`. This workflow is wrapped using `scripts/visualize_grid_evaluate.sh`:\n\n```\nUsage: visualize_grid_evaluate.sh [-h|--help] model_log_directory\n\nVisualize grid evaluations for SoPa++ and regex model pairs, given\nthe grid-search allows for the following varying arguments:\npatterns, tau_threshold, seed\n\nOptional arguments:\n  -h, --help                       Show this help message and exit\n\nRequired arguments:\n  model_log_directory <glob_path>  Model log directory/directories\n                                   containing SoPa++ and regex models,\n                                   as well as all evaluation json's\n```\n\nTo produce a facet-based visualization of grid-based evaluation, simply execute:\n\n```shell\nbash scripts/visualize_grid_evaluate.sh \"/glob/to/model/log/*/director(ies)\"\n```\n\n**Note:** This script has been hard-coded for grid-based evaluation scenarios where only the following three training/model arguments are varied: `patterns`, `tau_threshold` and `seed`.\n\n</p>\n</details>\n\n<details><summary>v. TauSTE neurons and RE samples</summary>\n<p>\n\nFor visualizing TauSTE neurons and RE samples, we use `src/visualize_regex.py`:\n\n```\nusage: visualize_regex.py [-h] --class-mapping-config <file_path>\n                          --regex-model-checkpoint <glob_path>\n                          [--disable-tqdm]\n                          [--logging-level {debug,info,warning,error,critical}]\n                          [--max-num-regex <int>]\n                          [--max-transition-tokens <int>] [--only-neurons]\n                          [--seed <int>] [--tqdm-update-period <int>]\n\noptional arguments:\n  -h, --help                show this help message and exit\n\nrequired visualization arguments:\n  --class-mapping-config    <file_path>\n                            Path to class mapping configuration (default:\n                            None)\n  --regex-model-checkpoint  <glob_path>\n                            Glob path to regex model checkpoint(s) with '.pt'\n                            extension (default: None)\n\noptional visualization arguments:\n  --max-num-regex           <int>\n                            Maximum number of regex's for each TauSTE neuron\n                            (default: 10)\n  --max-transition-tokens   <int>\n                            Maximum number of tokens to display per transition\n                            (default: 5)\n  --only-neurons            Only produces plots of neurons without regex's\n                            (default: False)\n  --seed                    <int>\n                            Random seed for numpy (default: 42)\n\noptional logging arguments:\n  --logging-level           {debug,info,warning,error,critical}\n                            Set logging level (default: info)\n\noptional progress-bar arguments:\n  --disable-tqdm            Disable tqdm progress bars (default: False)\n  --tqdm-update-period      <int>\n                            Specify after how many training updates should the\n                            tqdm progress bar be updated with model\n                            diagnostics (default: 5)\n```\n\nTo visualize TauSTE neurons and corresponding activating RE samples, execute the following:\n\n```shell\nbash scripts/visualize_regex_with_neurons.sh \"/glob/to/regex/model/*/checkpoint(s)\" \n```\n\nTo visualize only TauSTE neurons, execute the following:\n\n```shell\nbash scripts/visualize_regex_only_neurons.sh \"/glob/to/regex/model/*/checkpoint(s)\" \n```\n\n</p>\n</details>\n"}
{"url": "https://github.com/tscheypidi/mrorganic", "owner": "tscheypidi", "repository_name": "mrorganic", "date_all_variable_collection": "2023-07-04", "description": null, "size": 1855, "stargazers_count": 0, "watchers_count": 0, "language": "R", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 1, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "GNU Lesser General Public License v3.0", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 1, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "tscheypidi", "contributions": 63}, {"contributor": "pfuehrlich-pik", "contributions": 43}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": true, "howfairis_citation": true, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 31273}, {"language": "Makefile", "num_chars": 1899}], "readme": "# Provide soil organic carbon and biomass data\n\nR package **mrorganic**, version **0.5.6**\n\n[![CRAN status](https://www.r-pkg.org/badges/version/mrorganic)](https://cran.r-project.org/package=mrorganic)  [![R build status](https://github.com/tscheypidi/mrorganic/workflows/check/badge.svg)](https://github.com/tscheypidi/mrorganic/actions) [![codecov](https://codecov.io/gh/tscheypidi/mrorganic/branch/master/graph/badge.svg)](https://app.codecov.io/gh/tscheypidi/mrorganic) [![r-universe](https://pik-piam.r-universe.dev/badges/mrorganic)](https://pik-piam.r-universe.dev/builds)\n\n## Purpose and Functionality\n\nmrorganic supports downloading and preparing soil organic carbon\n    and biomass data for further scientific work.\n\n\n## Installation\n\nFor installation of the most recent package version an additional repository has to be added in R:\n\n```r\noptions(repos = c(CRAN = \"@CRAN@\", pik = \"https://rse.pik-potsdam.de/r/packages\"))\n```\nThe additional repository can be made available permanently by adding the line above to a file called `.Rprofile` stored in the home folder of your system (`Sys.glob(\"~\")` in R returns the home directory).\n\nAfter that the most recent version of the package can be installed using `install.packages`:\n\n```r \ninstall.packages(\"mrorganic\")\n```\n\nPackage updates can be installed using `update.packages` (make sure that the additional repository has been added before running that command):\n\n```r \nupdate.packages()\n```\n\n## Tutorial\n\nThe package comes with a vignette describing the basic functionality of the package and how to use it. You can load it with the following command (the package needs to be installed):\n\n```r\nvignette(\"mrorganic\") # mrorganic: a data aggregation package for soil organic carbon and biomass data\n```\n\n## Questions / Problems\n\nIn case of questions / problems please contact Jan Philipp Dietrich <dietrich@pik-potsdam.de>.\n\n## Citation\n\nTo cite package **mrorganic** in publications use:\n\nDietrich J, F\u00fchrlich P (2023). _mrorganic: Provide soil organic carbon and biomass data_. R package version 0.5.6, <URL: https://github.com/tscheypidi/mrorganic>.\n\nA BibTeX entry for LaTeX users is\n\n ```latex\n@Manual{,\n  title = {mrorganic: Provide soil organic carbon and biomass data},\n  author = {Jan Philipp Dietrich and Pascal F\u00fchrlich},\n  year = {2023},\n  note = {R package version 0.5.6},\n  url = {https://github.com/tscheypidi/mrorganic},\n}\n```\n"}
{"url": "https://github.com/mppalves/AgreenaRothC", "owner": "mppalves", "repository_name": "AgreenaRothC", "date_all_variable_collection": "2023-07-04", "description": "AgreenaRothC implementation", "size": 3449, "stargazers_count": 0, "watchers_count": 0, "language": "R", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 1, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 1, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "mppalves", "contributions": 58}, {"contributor": "hhau", "contributions": 1}, {"contributor": "timfernando", "contributions": 1}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 92491}]}
{"url": "https://github.com/mppalves/AgreenaRothC2", "owner": "mppalves", "repository_name": "AgreenaRothC2", "date_all_variable_collection": "2023-07-04", "description": null, "size": 1369, "stargazers_count": 0, "watchers_count": 0, "language": "R", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "mppalves", "contributions": 6}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 103208}]}
{"url": "https://github.com/mppalves/CalibValidRothC", "owner": "mppalves", "repository_name": "CalibValidRothC", "date_all_variable_collection": "2023-07-04", "description": null, "size": 606, "stargazers_count": 0, "watchers_count": 0, "language": "R", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 1, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 1, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "hhau", "contributions": 10}, {"contributor": "mppalves", "contributions": 5}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 400234}]}
{"url": "https://github.com/mppalves/CO2CityMap", "owner": "mppalves", "repository_name": "CO2CityMap", "date_all_variable_collection": "2023-07-04", "description": null, "size": 816, "stargazers_count": 0, "watchers_count": 0, "language": "R", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "mppalves", "contributions": 5}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 7253}], "readme": "# CO2CityMap\n\n### Description\n\nPackaged to process spatial-temporal data and calculate CO2 emissions from urban centers.\n"}
{"url": "https://github.com/mppalves/food-agriculture-policy", "owner": "mppalves", "repository_name": "food-agriculture-policy", "date_all_variable_collection": "2023-07-04", "description": "Repository created to store food and agriculture policy didactic models developed as course projects", "size": 1890, "stargazers_count": 0, "watchers_count": 0, "language": "Jupyter Notebook", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "mppalves", "contributions": 24}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Jupyter Notebook", "num_chars": 17520}, {"language": "R", "num_chars": 10655}], "readme": "# Food and Agriculture Policy\nThis is a repository created to store food and agriculture policy didactic models developed as course projects\n"}
{"url": "https://github.com/mppalves/Game-theory", "owner": "mppalves", "repository_name": "Game-theory", "date_all_variable_collection": "2023-07-04", "description": "Game theory model for international trade and tariffs simulation", "size": 8549, "stargazers_count": 0, "watchers_count": 0, "language": "Mathematica", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "mppalves", "contributions": 9}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Mathematica", "num_chars": 381400}]}
{"url": "https://github.com/mppalves/GSTools", "owner": "mppalves", "repository_name": "GSTools", "date_all_variable_collection": "2023-07-04", "description": "GSTools is a package with auxiliary functions used in may master thesis work", "size": 49, "stargazers_count": 0, "watchers_count": 0, "language": "R", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "mppalves", "contributions": 20}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 32924}], "readme": "# Grass Tools \n\n### Description\n\nGroup of functions and warpper functions to pre-process and analyse data from LPJmL\n\n### Details\n\nList of functions in this package:\n\n``` create_map\nexport_weights_gams()\n\ngenerate_test_data()\n\nOptimize_LSU()\n\nploting_individual_cells()\n\nreconstruct_dataset()\n\nRun_analysis()\n\nRun_simulation()\n```\n\n### Instalation\n\n```\ninstall.packages(\"devtools\")\nlibrary(devtools)\ndevtools::install_github(\"mppalves/GSTools\")\n```\n\n### Author\n\nMarcos Alves [mppalves@gmail.com](mppalves@f=gmail.com)\n"}
{"url": "https://github.com/mppalves/lpjmule", "owner": "mppalves", "repository_name": "lpjmule", "date_all_variable_collection": "2023-07-04", "description": null, "size": 673, "stargazers_count": 0, "watchers_count": 0, "language": "R", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": false, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "mppalves", "contributions": 17}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 12502}], "readme": "# LPJml-emulator\n Machine learning model to emulate grass harvest and soil carbon\n"}
{"url": "https://github.com/mppalves/NetLogo_Bacterial_Infection_Model", "owner": "mppalves", "repository_name": "NetLogo_Bacterial_Infection_Model", "date_all_variable_collection": "2023-07-04", "description": "Agent based model of inflammatory process", "size": 657, "stargazers_count": 0, "watchers_count": 0, "language": "NetLogo", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "mppalves", "contributions": 7}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "NetLogo", "num_chars": 36225}], "readme": "# NETLOGO BACTERIAL INFECTION MODEL\n## WHAT IS IT?\n\nThis model illustrates the dynamics of macrophages, bacteria, and mastocytes in inflammatory processes on vascularized epithelium tissue. It can be thought of as a cross-section of skin where invading bacteria feed of the healthy tissue causing damage while triggering the tissue\u2019s immune response.\n\nThe immune response is controlled by cytokines, a category of signaling molecules that mediate and regulate immunity, inflammation, and hematopoiesis. Cytokines are released when bacteria feed on the tissue or when a macrophage phagocytes a bacterium. The high concentration of cytokines creates a positive gradient that recruits more macrophages to an area, stimulates mitoses of macrophages, and the release of histamine by Mastocytes.\n\nIn this toy model, bacteria can evolve to try to avoid the immune system by either increasing the speed with which they move while sacrificing feed efficiency or vice-versa, depending on the environment determined by the infection state. Macrophages do not evolve but can have two different origins, native from the tissue (Mitotic macrophages) or recruited from the bloodstream (Blood macrophages), and play a crucial role in controlling the liberation of histamine by the mastocytes.\n\nIn the fight against an infection, the interaction between different defense mechanisms creates very interesting patterns and defense strategies, some of which can be explored in a simplified way in this model.\n\n### Screen shot\n\n![Screen shot](model_screen.png)\n\n## HOW IT WORKS\n\n#### Moviment\nThere are three main types of agents in this model, which are bacteria (visualized as colored ovals), mastocytes (visualized as big red doted circles), and macrophages (visualized as colored spiky circles with a black dot).\n\nBacteria wander the tissue randomly, destroying and eating epithelium and endothelial cells (visualized as brow and red patches, respectively) to gain energy as they move. In the absence of a cytokine gradient, macrophages move randomly in the healthy tissue and receive nutrition. When macrophages detect cytokines, they move toward the positive cytokine gradient. Mastocytes are fixed cells that do not die, multiply or need to feed. Bacteria and macrophages will die if they run out of energy, for example, by moving extensively on tissue that is severally damaged and low in nutrients.\n\nBacteria speed defines how fast they move, and feed efficiency determines how much energy the bacteria spend in each clock tick. Increasing speed is an effective way for bacteria to evade the immune defenses, but mutations that increase speed also reduce the feed efficiency as faster movement requires more energy per step. As a result, faster bacteria are more efficient to evade the immune system but are more likely to die by starvation.\n\n#### Multiplication and mutations\nBacteria will divide if they accumulate enough energy. When bacteria divide, they have a chance of mutating the speed and feed efficiency. The mutations are transmitted to the next generations. Bacteria can have different colors to express their level of mutation:\n\n* Light brown: slow speed, high feed efficiency.\n* Green: medium speed, medium feed efficiency.\n* Light blue: fast speed, low feed efficiency.\n\nMacrophages have a chance to undergo mitosis if they are amidst a large concentration of cytokines, originating pink macrophages (mitotic macrophages). Cytokines that fall in the blood vessel can also recruit macrophages from other tissues (blood macrophages) that are red. The two types of macrophages (pink and red) are functionally identical, and the color difference serves to track the type of defense (local or systemic) more active in each stage of the infection.\n\n\n#### Molecular mediators\nCytokines are released in small quantities when bacteria destroy healthy tissue and at large amounts when a macrophage phagocyte a bacterium. They are represented by the white-pinkish flashes released on tissue patches. Usually, the release of cytokines by dying tissue is enough to recruit nearby macrophages but not sufficient to trigger mastocytes' histamine release. The aim is to represent the crucial role that phagocytosis play in mediating other immune responses.\n\nHistamine (small red dots) released by mastocytes has a short duration on the tissue, but their role in the immune response is paramount. When bacteria feed off a patch that contains active histamine, it loses its mobility and becomes an easy target for macrophages or death by starvation.\n\n#### Agents deaths\n\nMacrophages die when they starve by moving throughout damaged tissue. Bacteria die when they are phagocyted by macrophages or die of starvation by moving in damaged tissue. Histamine disappears after the set duration time of its effect.\n\n## HOW TO USE IT\n\nThis model can be used to observe the interaction between immune cells and their immune mediators and possible outcomes of an infectious process. \n\n### Buttons\n#### Setup\nInitializes variables and creates the initial bacteria, macrophages, and mastocytes. \n#### Go\nRuns the model\n### Sliders and Switches \n#### Initial-mastocytes\nThis slider controls the number of mastocytes in the tissue at the beginning of the simulation.\n#### Number-ini-bacteria\nThis slider controls the number of bacteria infecting the tissue at the beginning of the simulation.\n#### Initial-num-macrophages\nThis slider controls the number of macrophages in the beginning of the simulation.\n#### Histamine-effect-duration\nThis slider defines for how long the released histamine will stay active in the tissue\n#### Initial-bacterias-speed\nThis slider defines the initial bacteria speed and its correspondent feed efficiency   \n#### Macrophage-speed\nThis slider defines the fixed macrophages speed\n#### Histamine-release-threshold\nThis slide determines the minimum concentration of cytokines needed to trigger the release of histamine by mastocytes.\n#### Mutation?\nSwitch on and off the possibility of bacteria to suffer mutation.\n\n### Plots and monitors\n\n#### Bacteria avg speed - Monitor\nThis monitor shows the current average bacterial speed.\n\n#### Bacteria avg speed - Plot\nThis plot shows the evolution of bacterial speed over time.\n\n#### Bacteria avg feed efficiency\nThis plot shows the evolution of bacterial feed efficiency over time as a result of mutation and selection pressure.\n\n#### Tissue population of bacteria and immune cells\nPlots the total cell number for each type of cell (bacteria from the 3 possible types and macrophages from the 2 different origins) \n\n#### Anaphylactic bursts\nPlots the number of active histamine molecules over time.\n\n#### Tissue Health\nPlots the mean amount of food available for bacteria and macrophages. The measurement of tissue food can be understood as a proxy for its health status\n\n\n## THINGS TO NOTICE\n\n\nWhen clicking in \"go\" take some time to watch what is happening in the simulation. Notice how the cytokines signals draw macrophages to where the bacteria are. After successful phagocytosis, observe how the macrophages amplify the cytokines signal triggering other responses such as the mitosis and recruitment of other macrophages and the bursts of histamine by mastocytes. Is this kind of interaction and amplification of signals a common feature in the immune responses? In a living organism, is it true that the larger the immune response to infection, the better?\n\nAfter a while of playing with the simulation, you will probably observe scenarios where the immune system subsides the infection and others where the infection becomes chronic. What are the relevant factors that determine these two possible outcomes? Is it possible to quell the infection by allowing either macrophages or mastocytes to defend the tissue alone? What are the conditions for that? Is it viable in a living organism?\n\nThe macrophages and bacteria depend on living tissue to thrive. Both feed on the nutrients available there. After the installation of a chronic infection, what happens with the overall tissue health (available food)? Does that favor bacteria or macrophages? Does it resemble what happens in reality?\n\nAs time goes on, observe how the bacterial population changes and how the number of defense cells and histamine release fluctuates. Is there a relationship between tissue health, histamine releases, and the number of macrophages?\n\n\n## THINGS TO TRY\n\nTry changing the initial number of bacterias (bacterial load) to see how that affects the speed and probability of a severe infection.\n\nAnother interesting possibility is to observe how natural selection affects the predominant bacteria. Try to switch on and off the mutation knob to see how that affects the chances of the infection to thrive. \n\nPlay with the different histamine characteristics (release threshold and duration). \n\n## EXTENDING THE MODEL\n\nTry adding other immune cells and signaling molecules such as lymphocytes and interleukins, respectively. \n\n## DISCLAIMER\n\nThis model was developed as a course requirement. The focus was to develop skills and understanding of agent-based modeling. Therefore,  the data, the description of the interaction between cells, and the biological variables mentioned here are by no means validated. Any conclusion about bacterial infections taken from this simulation will probably be incomplete or wrong.\n\n## CREDITS AND REFERENCES\n\nCode in this model made use of several solutions found in the models below:\n\n* Woods, P. and Wilensky, U. (2019). NetLogo CRISPR Ecosystem model. http://ccl.northwestern.edu/netlogo/models/CRISPREcosystem. Center for Connected Learning and Computer-Based Modeling, Northwestern University, Evanston, IL.\n\n* Wilensky, U. (1997). NetLogo Ants model. http://ccl.northwestern.edu/netlogo/models/Ants. Center for Connected Learning and Computer-Based Modeling, Northwestern University, Evanston, IL.\n\n* Dunham, G., Tisue, S. and Wilensky, U. (2004). NetLogo Erosion model. http://ccl.northwestern.edu/netlogo/models/Erosion. Center for Connected Learning and Computer-Based Modeling, Northwestern University, Evanston, IL.\n\n## HOW TO CITE\n\nIf you mention this model or the NetLogo software in a publication, we ask that you include the citations below.\n\nFor the model itself:\n\n* Alves, Marcos (2021).  NetLogo Bacterial Infection Model.  https://github.com/mppalves/NetLogo_Bacterial_Infection_Model.\n\nPlease cite the NetLogo software as:\n\n* Wilensky, U. (1999). NetLogo. http://ccl.northwestern.edu/netlogo/. Center for Connected Learning and Computer-Based Modeling, Northwestern University, Evanston, IL.\n\n"}
{"url": "https://github.com/mppalves/PIK-projects", "owner": "mppalves", "repository_name": "PIK-projects", "date_all_variable_collection": "2023-07-04", "description": "Files related to my Master thesis research", "size": 180935, "stargazers_count": 0, "watchers_count": 0, "language": "Mathematica", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "mppalves", "contributions": 19}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Mathematica", "num_chars": 1595986}, {"language": "R", "num_chars": 110979}, {"language": "GAMS", "num_chars": 5531}], "readme": "# PIK-projects\n### Description\n\nFiles related to the Master Thesis research on the effects of pasture management on biophysical and social indications.\n\n### UML\n![UML](Methodology%20-%20Master%20Thesis-UML.jpg)\n\n### GStools\n\nAll functions and files presented in \"File bundles\" were aggruped in the R package [GSTools](https://github.com/mppalves/GSTools) for better management.\n\n### Author\nMarcos Alves [mppalves@gmail.com](mppalves@f=gmail.com)\n"}
{"url": "https://github.com/mppalves/soilcemulator", "owner": "mppalves", "repository_name": "soilcemulator", "date_all_variable_collection": "2023-07-04", "description": null, "size": 138, "stargazers_count": 0, "watchers_count": 0, "language": "R", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "GNU Lesser General Public License v3.0", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "mppalves", "contributions": 55}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": true, "howfairis_citation": true, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 41505}], "readme": "# Soil carbon pasture emulator creator\n\nR package **soilcemulator**, version **2.4.0.9001**\n\n[![CRAN status](https://www.r-pkg.org/badges/version/soilcemulator)](https://cran.r-project.org/package=soilcemulator)   [![R build status](https://github.com/mppalves/mrscpmule/workflows/check/badge.svg)](https://github.com/mppalves/mrscpmule/actions) [![codecov](https://codecov.io/gh/mppalves/mrscpmule/branch/master/graph/badge.svg)](https://codecov.io/gh/mppalves/mrscpmule)\n\n## Purpose and Functionality\n\nProvides functions for the creation of a Machine learning emulator based on magpie preprocessing files.\n\n\n## Installation\n\nFor installation of the most recent package version an additional repository has to be added in R:\n\n```r\noptions(repos = c(CRAN = \"@CRAN@\", pik = \"https://rse.pik-potsdam.de/r/packages\"))\n```\nThe additional repository can be made available permanently by adding the line above to a file called `.Rprofile` stored in the home folder of your system (`Sys.glob(\"~\")` in R returns the home directory).\n\nAfter that the most recent version of the package can be installed using `install.packages`:\n\n```r \ninstall.packages(\"soilcemulator\")\n```\n\nPackage updates can be installed using `update.packages` (make sure that the additional repository has been added before running that command):\n\n```r \nupdate.packages()\n```\n\n## Questions / Problems\n\nIn case of questions / problems please contact Alves Marcos <pedrosa@pik-potsdam.de>.\n\n## Citation\n\nTo cite package **soilcemulator** in publications use:\n\nMarcos A (2021). _soilcemulator: Soil carbon pasture emulator creator_. https://github.com/mppalves/mrscpmule,.\n\nA BibTeX entry for LaTeX users is\n\n ```latex\n@Manual{,\n  title = {soilcemulator: Soil carbon pasture emulator creator},\n  author = {Alves Marcos},\n  year = {2021},\n  note = {https://github.com/mppalves/mrscpmule,},\n}\n```\n\n"}
{"url": "https://github.com/mppalves/Syngenta_challenge", "owner": "mppalves", "repository_name": "Syngenta_challenge", "date_all_variable_collection": "2023-07-04", "description": null, "size": 1587, "stargazers_count": 0, "watchers_count": 0, "language": "HTML", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "mppalves", "contributions": 9}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "HTML", "num_chars": 944150}, {"language": "R", "num_chars": 11271}], "readme": "# Syngenta crop challenge 2019\n\nThe project files here presented were developed to participate in the syngenta crop challanged 2019 described bellow.\nThe database is not provided to respect the participation agreement with syngenta.\n\n## Description:\nToday, the agriculture industry works to maximize the amount of food we gain from crops by breeding plants with the strongest, highest-yielding genetics. Scientists at research and development organizations like Syngenta create more resilient plants by breeding and then selecting the best offspring, over time, to provide to farmers.\n\nWe\u2019ve proven that data-driven strategies can help our industry breed better seeds that require fewer resources and are adaptable to more diverse environments. Developing models that identify robust patterns in our experimental data may help scientists more accurately choose seeds that increase the productivity of the crops we plant \u2013 and will help address the growing global food demand.\n\n"}
{"url": "https://github.com/k4rst3ns/historicalsocbudget", "owner": "k4rst3ns", "repository_name": "historicalsocbudget", "date_all_variable_collection": "2023-07-04", "description": "Paper for Historical SOC budget including result notebooks paperdraft as Rmd and PDF", "size": 69605, "stargazers_count": 0, "watchers_count": 0, "language": "TeX", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "k4rst3ns", "contributions": 96}, {"contributor": "hackmd-deploy", "contributions": 31}, {"contributor": "bodirsky", "contributions": 14}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "TeX", "num_chars": 1140093}], "readme": "# Manuscript and output processing of the paper: \"Management induced changes of soil organic carbon on global croplands\"\nThis scripts contains:\n* manuscript of the Paper \"Management induced changes of soil organic carbon on global croplands\"\n* output postprocessing scripts that transform model output data into output figures and tables\n\n### REFERENCES\n\nKarstens, Kristine. (2022). Model output data of the paper: \"Management induced changes of soil organic carbon on global croplands\" [Data set]. Zenodo. https://doi.org/10.5281/zenodo.6330102\n\nDietrich J, Baumstark L, Wirth S, Giannousakis A, Rodrigues R, Bodirsky B, Kreidenweis U, Klein D, F\u00fchrlich P (2022). _madrat: May All Data be Reproducible and Transparent (MADRaT)_. doi: 10.5281/zenodo.1115490 (URL: https://doi.org/10.5281/zenodo.1115490), R package version 2.13.0, <URL: https://github.com/pik-piam/madrat>.\n\nKarstens K, Dietrich J (2022). _mrsoil: MadRat Soil Organic Carbon Budget Library_. doi: 10.5281/zenodo.4317933 (URL: https://doi.org/10.5281/zenodo.4317933), R package version 1.17.11, <URL: https://github.com/pik-piam/mrsoil>.\n\nBodirsky B, Karstens K, Baumstark L, Weindl I, Wang X, Mishra A, Wirth S, Stevanovic M, Steinmetz N, Kreidenweis U, Rodrigues R, Popov R, Humpenoeder F, Giannousakis A, Levesque A, Klein D, Araujo E, Beier F, Oeser J, Pehl M, Leip D, Crawford M, Molina Bacca E, von Jeetze P, Martinelli E, Schreyer F, Soergel B, F\u00fchrlich P, Dietrich J (2022). _mrcommons: MadRat commons Input Data Library_. doi: 10.5281/zenodo.3822009 (URL: https://doi.org/10.5281/zenodo.3822009), R package version 1.7.14, <URL: https://github.com/pik-piam/mrcommons>.\n\nKarstens K, Dietrich J, Chen D, Windisch M, Alves M, Beier F, v. Jeetze P, Mishra A, Humpenoeder F (2022). mrmagpie: madrat based MAgPIE Input Data Library. doi: 10.5281/zenodo.4319612 (URL: https://doi.org/10.5281/zenodo.4319612), R package version 1.21.15, <URL: https://github.com/pik-piam/mrmagpie>.\n\nBodirsky B, Wirth S, Karstens K, Humpenoeder F, Stevanovic M, Mishra A, Biewald A, Weindl I, Beier F, Chen D, Crawford M, Molina Bacca E, Kreidenweis U, W. Yalew A, Humpenoeder F, von Jeetze P, Wang X, Dietrich J, Alves M (2022). _mrvalidation: madrat data preparation for validation purposes_. doi: 10.5281/zenodo.4317826 (URL: https://doi.org/10.5281/zenodo.4317826), R package version 2.36.8, <URL: https://github.com/pik-piam/mrvalidation>.\n\n## LICENSE\nThis data is open-source: you can redistribute it and/or modify it under the terms of the **CC Attribution 4.0 International** as published by the Creative Commons Corporation at https://creativecommons.org/licenses/by/4.0/legalcode.\n\n## CONTACT\nkarstens@pik-potsdam.de \n"}
{"url": "https://github.com/johanneskoch94/drat", "owner": "johanneskoch94", "repository_name": "drat", "date_all_variable_collection": "2023-07-04", "description": null, "size": 44, "stargazers_count": 0, "watchers_count": 0, "language": "HTML", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": true, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "johanneskoch94", "contributions": 8}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "HTML", "num_chars": 36}]}
{"url": "https://github.com/johanneskoch94/myUtils", "owner": "johanneskoch94", "repository_name": "myUtils", "date_all_variable_collection": "2023-07-04", "description": "Tools to analyze REMIND results", "size": 43, "stargazers_count": 0, "watchers_count": 0, "language": "R", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 1, "license": "GNU General Public License v3.0", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 1, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "johanneskoch94", "contributions": 19}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 21194}], "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# myUtils\n\n<!-- badges: start -->\n<!-- badges: end -->\n\nBits and bobs to help analyze REMIND results\n\n## Installation\n\nInstall the development version of myUtils from Github:\n\n``` r\nremotes::install_github(\"johanneskoch94/myUtils\")\n```\n\n## Example\n\nUse `explore_CEStree` to open a shiny app and compare CES parameters\n(pm\\_cesData) and final values of the production factors (vm\\_ces) of\nREMIND runs.\n\n``` r\nlibrary(myUtils)\nexplore_CEStree(c(\"path1/fulldata.gdx\", \"path2/fulldata.gdx\"))\n```\n"}
{"url": "https://github.com/johanneskoch94/remindStart", "owner": "johanneskoch94", "repository_name": "remindStart", "date_all_variable_collection": "2023-07-04", "description": "R-package to start REMIND", "size": 467, "stargazers_count": 0, "watchers_count": 0, "language": "R", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": true, "has_discussions": false, "forks_count": 1, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 1, "license": "Other", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 1, "open_issues": 1, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "johanneskoch94", "contributions": 25}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": true, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 124508}], "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# remindStart\n\n<!-- badges: start -->\n<!-- badges: end -->\n\nWith remindStart, you can \u2026 start REMIND!\n\n## Installation\n\nYou can install the development version of remindStart like so:\n\n``` r\nremotes::install_github(\"johanneskoch94/remindStart\")\n```\n\n## Start REMIND from the command line\n\nIf your working directory is the remind directory:\n\n``` r\n# Default cfg\nRscript -e \"remindStart::start()\" \n\n# Pass a scenario config file\nRscript -e \"remindStart::start()\" --args path/to/config_file.csv\nRscript -e \"remindStart::start(configFile = 'path/to/config_file.csv')\"\n\n# Restart\nRscript -e \"remindStart::start()\" --args restart\nRscript -e \"remindStart::start(restart = TRUE)\"\n\n# Test one region\nRscript -e \"remindStart::start()\" --args testOneRegi\nRscript -e \"remindStart::start(testOneRegi = TRUE)\"\n```\n\nIf your working directory is not the remind directory, specify the path\nwith the `remind` argument:\n\n``` r\nRscript -e \"remindStart::start(remind = 'path/to/remind')\" \n```\n\n## Start REMIND from within R\n\n``` r\nremindStart::start(configFile = \"path/to/config_file.csv\")\nremindStart::start(remind = \"path/to/remind\", configFile = \"path/to/config_file.csv\")\nremindStart::start(restart = TRUE)\n```\n"}
{"url": "https://github.com/aodenweller/green-h2-upscaling", "owner": "aodenweller", "repository_name": "green-h2-upscaling", "date_all_variable_collection": "2023-07-04", "description": "Model code and input data of the technology diffusion model for electrolysis capacity", "size": 3721, "stargazers_count": 5, "watchers_count": 5, "language": "R", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 4, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 4, "open_issues": 0, "watchers": 5, "default_branch": "master", "contributors": [{"contributor": "aodenweller", "contributions": 5}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 53106}], "readme": "# Green hydrogen upscaling\n\nThis repository contains the full model code to run the technology diffusion simulation and reproduce all figures of the article:\n\nOdenweller, A., Ueckerdt, F., Nemet, G. F., Jensterle, M., and Luderer, G.: *Probabilistic feasibility space of scaling up green hydrogen supply*\n\n## Contents\n* `main.Rmd` - The main R Markdown file\n* `01_input_data` - Input data files\n* `02_output_plots` - Output folder for simulation data and plots\n* `03_functions` - Helper functions\n* `04_plotting` - Plotting functions\n\n## Instructions\nThere are two options to use this code, which are controlled by a switch in `main.Rmd`\n* Reproduction mode (default)\n  * Download the [pre-run simulation output from Zenodo](https://doi.org/10.5281/zenodo.6567669) and put all files into `02_output_plots`\n  * Run `main.Rmd`\n  * This imports the simulation output, reproduces all article figures, and saves them into `02_output_plots` \n* Simulation mode\n  * Set `model.mode = \"simulation\"` in `main.Rmd`\n  * Adjust the sample size `n` of the simulation as desired\n  * Run `main.Rmd`\n  * This starts the simulation, which will take some time, depending on the sample size\n  * Figures are saved into `02_output_plots` and might deviate slightly from article figures due to randomness"}
{"url": "https://github.com/aodenweller/remind-pypsa", "owner": "aodenweller", "repository_name": "remind-pypsa", "date_all_variable_collection": "2023-07-04", "description": "Scripts for the REMIND-PyPSA model coupling", "size": 0, "stargazers_count": 0, "watchers_count": 0, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "develop", "contributors": [{"contributor": "aodenweller", "contributions": 1}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": []}
{"url": "https://github.com/abhimishr/abhimishr", "owner": "abhimishr", "repository_name": "abhimishr", "date_all_variable_collection": "2023-07-04", "description": null, "size": 42, "stargazers_count": 0, "watchers_count": 0, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "abhimishr", "contributions": 17}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [], "readme": "\n"}
{"url": "https://github.com/abhimishr/appIMPACT", "owner": "abhimishr", "repository_name": "appIMPACT", "date_all_variable_collection": "2023-07-04", "description": null, "size": 18, "stargazers_count": 0, "watchers_count": 0, "language": "R", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": false, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "abhimishr", "contributions": 8}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 9879}]}
{"url": "https://github.com/abhimishr/dynamic_forestry_GitHub", "owner": "abhimishr", "repository_name": "dynamic_forestry_GitHub", "date_all_variable_collection": "2023-07-04", "description": null, "size": 11204, "stargazers_count": 1, "watchers_count": 1, "language": "GAMS", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "Other", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 1, "default_branch": "master", "contributors": [{"contributor": "tscheypidi", "contributions": 637}, {"contributor": "abhimishr", "contributions": 528}, {"contributor": "flohump", "contributions": 369}, {"contributor": "bodirsky", "contributions": 294}, {"contributor": "k4rst3ns", "contributions": 210}, {"contributor": "mishkos", "contributions": 181}, {"contributor": "dklein-pik", "contributions": 89}, {"contributor": "xwangatpik", "contributions": 62}, {"contributor": "weindl", "contributions": 61}, {"contributor": "Geambrosio", "contributions": 37}, {"contributor": "araujoe", "contributions": 36}, {"contributor": "FelicitasBeier", "contributions": 31}, {"contributor": "caviddhen", "contributions": 4}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": true, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "GAMS", "num_chars": 955946}, {"language": "R", "num_chars": 240715}, {"language": "Shell", "num_chars": 187}], "readme": "# MAgPIE - Modular open source framework for modeling global land-systems\n\n  <https://www.pik-potsdam.de/research/projects/activities/land-use-modelling/magpie>\n\n## WHAT IS MAGPIE?\nThe *Model of Agricultural Production and its Impact on the Environment* (MAgPIE)\nis a modular open source framework for modeling global land-systems, which is \ncoupled to the grid-based dynamic vegetation model LPJmL, with a spatial resolution \nof 0.5\u00b0x0.5\u00b0. It takes regional economic conditions such as demand for agricultural \ncommodities, technological development and production costs as well as spatially \nexplicit data on potential crop yields, land and water constraints (from LPJmL) into \naccount. Based on these, the model derives specific land use patterns, yields and \ntotal costs of agricultural production for each grid cell. The objective function of \nthe land use model is to minimize total cost of production for a given amount of \nregional food and bioenergy demand. Regional food energy demand is defined for an \nexogenously given population in 10 food energy categories, based on regional diets. \nFuture trends in food demand are derived from a cross-country regression analysis, \nbased on future scenarios on GDP and population growth.\n\n## DOCUMENTATION\nThe model documentation for version 4 can be found at\nhttps://rse.pik-potsdam.de/doc/magpie/version4/\n\nA most recent version of the documentation can also be extracted from the\nmodel source code via the R package goxygen\n(https://github.com/pik-piam/goxygen). To extract the documentation, install the\npackage and run the main function (goxygen) in the main folder of the model.\nThe resulting documentation can be found in the folder \"doc\".\n\nPlease pay attentions to the MAgPIE Coding Etiquette when you modify the code.\nThe Coding Etiquette you find at\nhttps://redmine.pik-potsdam.de/projects/pik-model-operations/wiki/Coding_Etiquette\nThe Coding Etiquette explains also the used name conventions and other\nstructural characteristics.\n\n## COPYRIGHT\nCopyright 2008-2018 Potsdam Institute for Climate Impact Research (PIK)\n\n## LICENSE\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the **GNU Affero General Public License** as published by\nthe Free Software Foundation, **version 3** of the License or later. You should\nhave received a copy of the GNU Affero General Public License along with this\nprogram. See the LICENSE file in the root directory. If not, see\nhttp://www.gnu.org/licenses/\n\n## NOTES\nBesides distribution and software-as-a-service applications the source code\nshould also be made available in the events of model based publications or\nmodel-based consulting.\n\nWhen using a modified version of **MAgPIE** which is not identical to versions\nin the official main repository at https://github.com/magpiemodel add a suffix\nto the name to allow distinguishing versions (format **MAgPIE-suffix**).\n\n## SYSTEM REQUIREMENTS\nThe model is quite resource heavy and works best on machines with high CPU clock\nand memory. Recommended is a machine with Windows, MacOS or Linux, with at least\n16GB of memory and a Core i7 CPU or similar.\n\n## HOW TO INSTALL\nMAgPIE requires *GAMS* (https://www.gams.com/) including licenses for the\nsolvers *CONOPT* and (optionally) *CPLEX* for its core calculations. As the model\nbenefits significantly of recent improvements in *GAMS* and *CONOPT4* it is\nrecommended to work with the most recent versions of both.\nPlease make sure that the GAMS installation path is added to the PATH variable\nof the system.\n\nIn addition *R* (https://www.r-project.org/) is required for pre- and\npostprocessing and run management (needs to be added to the PATH variable\nas well).\n\nFor R some packages are required to run MAgPIE. All except of one (`gdxrrw`) are\neither distributed via the offical R CRAN or via a separate repository hosted at\nPIK (PIK-CRAN). Before proceeding PIK-CRAN should be added to the list of\navailable repositories via:\n```\noptions(repos = c(CRAN = \"@CRAN@\", pik = \"https://rse.pik-potsdam.de/r/packages\"))\n```\n\nThe `gdxrrw` package has to be downloaded directly from GAMS via\n```\ndownload.file(\"https://support.gams.com/_media/gdxrrw:gdxrrw_1.0.2.zip\",\n              \"gdxrrw_1.0.2.zip\")\ninstall.packages(\u201creshape2\u201d)\ninstall.packages(\"gdxrrw_1.0.2.zip\",repos = NULL)\n```\nIn some cases it can happen that `gdxrrw` does not return an error message during\ninstallation but also did not install properly. To verify a successful\ninstallation try to load the package via `library(gdxrrw)`.\n\n--------------------------------------------------------------------------------\n\nIf loading of the package fails you need to install the package from source.\nUnder Windows this requires to install Rtools\n(https://cran.r-project.org/bin/windows/Rtools/) and to add it to the PATH\nvariable. After that you can run the following lines of code:\n\n```\ndownload.file(\"https://support.gams.com/_media/gdxrrw:gdxrrw_1.0.2.tar.gz\",\n              \"gdxrrw_1.0.2.tar.gz\")\ninstall.packages(\"gdxrrw_1.0.2.tar.gz\",repos = NULL, type=\"source\")\n```\n\n--------------------------------------------------------------------------------\n\n\nAfter that all remaining packages can be installed via `install.packages`\n\n```\npkgs <- c(\"ggplot2\",\n          \"curl\",\n          \"gdx\",\n          \"magclass\",\n          \"madrat\",\n          \"mip\",\n          \"lucode\",\n          \"magpie4\",\n          \"magpiesets\",\n          \"lusweave\",\n          \"luscale\",\n          \"goxygen\",\n\t\t  \"luplot\")\ninstall.packages(pkgs)\n```\nFor post-processing model outputs *Latex* is required\n(https://www.latex-project.org/get/). To be seen by the model it also needs to\nadded to the PATH variable of your system.\n\n## HOW TO CONFIGURE\nModel run settings are set in `config/default.cfg` (or another config file of\nthe same structure). New model scenarios can be created by adding a column to\n`config/scenario_config.csv`\n\n## HOW TO RUN\nTo run the model execute `Rscript start.R` (or `source(\"start.R\")` from within\nR) in the main folder of the model.\nThis will give you a list of available run scripts you can choose from. You can\nalso add your own run scripts by saving them in the folder scripts/start. To run\na single model run with settings as stated in default.cfg you can choose start\nscript \"default\". Make sure that the config file has been set correctly before\nstarting the model.\n\n## HOW TO CONTRIBUTE\nWe are interested in working with you! Just contact us through GitHub\n(https://github.com/magpiemodel) or by mail (magpie@pik-potsdam.de) if you have\nfound and/or fixed a bug, developed a new model feature, have ideas for further\nmodel development, suggestions for improvements or anything else. We are open to\nany kind of contribution. Our aim is to develop an open, transparent and\nmeaningful model of the agricultural land use sector to get a better\nunderstanding of the underlying processes and possible futures. Join us doing\nso!\n\n## DEPENDENCIES\nModel dependencies **must be publicly available** and should be Open Source.\nDevelopment aim is to rather minimize than expand dependencies on non-free\nand/or non open source software. That means that besides currently existing\ndependencies on GAMS, the GDXRRW R package and the corresponding solvers there\nshould be no additional dependencies of this kind and that these existing\ndependencies should be resolved in the future if possible.\n\nIf a new R package is added as dependency this package should fulfill the\nfollowing requirements:\n* The package is published under an Open Source license\n* The package is distributed through CRAN or PIK-CRAN (the PIK-based,\n  but publicly available package repository).\n* The package source code is available through a public, version controlled\n  repository such as GitHub\n\nFor other dependencies comparable measures should apply. When a dependency is\nadded this dependency should be added to the *HOW TO INSTALL* section in the\nREADME file of the model framework (mentioning the depencendy and explaining\nhow it can be installed). If not all requirements can be fulfilled by the new\ndependency this case should be discussed with the model maintainer\n(magpie@pik-potsdam.de) to find a good solution for it.\n\n## INPUT DATA\n\nIn order to allow other researchers to reproduce and use work done with MAgPIE\none needs to make sure that all components necessary to perform a run can be\nshared. One of these components is the input data. As proprietary data usually\ndoes not allow its free distribution it should generally be avoided.\n\nWhen adding a new data source, make sure that it can be freely shared with\nothers. If this is not the case please consider using a different source or\nsolution.\n\nData preparation should ideally be performed with the **madrat** data processing\nframework (https://github.com/pik-piam/madrat). This makes sure that the\nprocessing is reproducible and links properly to the already existing data\nprocessing for MAgPIE.\n\nIn case that these recommendations can not be followed we would be happy if you\ncould discuss that issue with the MAgPIE development team\n(magpie@pik-potsdam.de).\n\n## CONTACT\nmagpie@pik-potsdam.de\n\n## KNOWN BUGS\n\n## TROUBLESHOOTING\nPlease contact magpie@pik-potsdam.de\n\n## CITATION\nSee file CITATION.cff or the documentation of the model for information how\nto cite the model.\n\n[![DOI](https://zenodo.org/badge/135430060.svg)](https://zenodo.org/badge/latestdoi/135430060)\n\n## AUTHORS\nSee list of authors in CITATION.cff\n\n## CHANGELOG\nSee log on GitHub (https://github.com/magpiemodel)\n"}
{"url": "https://github.com/abhimishr/FADNUGent", "owner": "abhimishr", "repository_name": "FADNUGent", "date_all_variable_collection": "2023-07-04", "description": "FADN Data analysis from master thesis of Abhijeet Mishra @UGent, Belgium", "size": 12, "stargazers_count": 0, "watchers_count": 0, "language": "R", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "abhimishr", "contributions": 6}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 35348}], "readme": "# FADNUGent\nFADN Data analysis from master thesis of Abhijeet Mishra @UGent, Belgium\n\nDOI: 10.5281/zenodo.3551156\n"}
{"url": "https://github.com/abhimishr/faustmann_gams", "owner": "abhimishr", "repository_name": "faustmann_gams", "date_all_variable_collection": "2023-07-04", "description": "Calculating rotations lengths according to Faustmann criterion", "size": 21, "stargazers_count": 0, "watchers_count": 0, "language": "GAMS", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "GNU General Public License v3.0", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "abhimishr", "contributions": 5}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "GAMS", "num_chars": 3771}], "readme": "# faustmann_gams\nCalculating rotations lengths according to Faustmann criterion\n"}
{"url": "https://github.com/abhimishr/MAgPIESyntaxNpp", "owner": "abhimishr", "repository_name": "MAgPIESyntaxNpp", "date_all_variable_collection": "2023-07-04", "description": "MAgPIE model Syntax highlighting for GAMS (Notepad++ )", "size": 27, "stargazers_count": 3, "watchers_count": 3, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 1, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "GNU General Public License v3.0", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 1, "open_issues": 0, "watchers": 3, "default_branch": "master", "contributors": [{"contributor": "abhimishr", "contributions": 7}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [], "readme": "# MAgPIESyntaxNpp\nMAgPIE model Syntax highlighting for GAMS (Notepad++ )\n"}
{"url": "https://github.com/abhimishr/magpie_forestry", "owner": "abhimishr", "repository_name": "magpie_forestry", "date_all_variable_collection": "2023-07-04", "description": null, "size": 10612, "stargazers_count": 0, "watchers_count": 0, "language": "GAMS", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "Other", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "tscheypidi", "contributions": 637}, {"contributor": "abhimishr", "contributions": 528}, {"contributor": "flohump", "contributions": 369}, {"contributor": "bodirsky", "contributions": 294}, {"contributor": "k4rst3ns", "contributions": 210}, {"contributor": "mishkos", "contributions": 181}, {"contributor": "dklein-pik", "contributions": 89}, {"contributor": "xwangatpik", "contributions": 62}, {"contributor": "weindl", "contributions": 61}, {"contributor": "Geambrosio", "contributions": 37}, {"contributor": "araujoe", "contributions": 36}, {"contributor": "FelicitasBeier", "contributions": 31}, {"contributor": "caviddhen", "contributions": 4}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": true, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "GAMS", "num_chars": 955946}, {"language": "R", "num_chars": 240715}, {"language": "Shell", "num_chars": 187}], "readme": "# MAgPIE - Modular open source framework for modeling global land-systems\n\n  <https://www.pik-potsdam.de/research/projects/activities/land-use-modelling/magpie>\n\n## WHAT IS MAGPIE?\nThe *Model of Agricultural Production and its Impact on the Environment* (MAgPIE)\nis a modular open source framework for modeling global land-systems, which is \ncoupled to the grid-based dynamic vegetation model LPJmL, with a spatial resolution \nof 0.5\u00b0x0.5\u00b0. It takes regional economic conditions such as demand for agricultural \ncommodities, technological development and production costs as well as spatially \nexplicit data on potential crop yields, land and water constraints (from LPJmL) into \naccount. Based on these, the model derives specific land use patterns, yields and \ntotal costs of agricultural production for each grid cell. The objective function of \nthe land use model is to minimize total cost of production for a given amount of \nregional food and bioenergy demand. Regional food energy demand is defined for an \nexogenously given population in 10 food energy categories, based on regional diets. \nFuture trends in food demand are derived from a cross-country regression analysis, \nbased on future scenarios on GDP and population growth.\n\n## DOCUMENTATION\nThe model documentation for version 4 can be found at\nhttps://rse.pik-potsdam.de/doc/magpie/version4/\n\nA most recent version of the documentation can also be extracted from the\nmodel source code via the R package goxygen\n(https://github.com/pik-piam/goxygen). To extract the documentation, install the\npackage and run the main function (goxygen) in the main folder of the model.\nThe resulting documentation can be found in the folder \"doc\".\n\nPlease pay attentions to the MAgPIE Coding Etiquette when you modify the code.\nThe Coding Etiquette you find at\nhttps://redmine.pik-potsdam.de/projects/pik-model-operations/wiki/Coding_Etiquette\nThe Coding Etiquette explains also the used name conventions and other\nstructural characteristics.\n\n## COPYRIGHT\nCopyright 2008-2018 Potsdam Institute for Climate Impact Research (PIK)\n\n## LICENSE\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the **GNU Affero General Public License** as published by\nthe Free Software Foundation, **version 3** of the License or later. You should\nhave received a copy of the GNU Affero General Public License along with this\nprogram. See the LICENSE file in the root directory. If not, see\nhttp://www.gnu.org/licenses/\n\n## NOTES\nBesides distribution and software-as-a-service applications the source code\nshould also be made available in the events of model based publications or\nmodel-based consulting.\n\nWhen using a modified version of **MAgPIE** which is not identical to versions\nin the official main repository at https://github.com/magpiemodel add a suffix\nto the name to allow distinguishing versions (format **MAgPIE-suffix**).\n\n## SYSTEM REQUIREMENTS\nThe model is quite resource heavy and works best on machines with high CPU clock\nand memory. Recommended is a machine with Windows, MacOS or Linux, with at least\n16GB of memory and a Core i7 CPU or similar.\n\n## HOW TO INSTALL\nMAgPIE requires *GAMS* (https://www.gams.com/) including licenses for the\nsolvers *CONOPT* and (optionally) *CPLEX* for its core calculations. As the model\nbenefits significantly of recent improvements in *GAMS* and *CONOPT4* it is\nrecommended to work with the most recent versions of both.\nPlease make sure that the GAMS installation path is added to the PATH variable\nof the system.\n\nIn addition *R* (https://www.r-project.org/) is required for pre- and\npostprocessing and run management (needs to be added to the PATH variable\nas well).\n\nFor R some packages are required to run MAgPIE. All except of one (`gdxrrw`) are\neither distributed via the offical R CRAN or via a separate repository hosted at\nPIK (PIK-CRAN). Before proceeding PIK-CRAN should be added to the list of\navailable repositories via:\n```\noptions(repos = c(CRAN = \"@CRAN@\", pik = \"https://rse.pik-potsdam.de/r/packages\"))\n```\n\nThe `gdxrrw` package has to be downloaded directly from GAMS via\n```\ndownload.file(\"https://support.gams.com/_media/gdxrrw:gdxrrw_1.0.2.zip\",\n              \"gdxrrw_1.0.2.zip\")\ninstall.packages(\u201creshape2\u201d)\ninstall.packages(\"gdxrrw_1.0.2.zip\",repos = NULL)\n```\nIn some cases it can happen that `gdxrrw` does not return an error message during\ninstallation but also did not install properly. To verify a successful\ninstallation try to load the package via `library(gdxrrw)`.\n\n--------------------------------------------------------------------------------\n\nIf loading of the package fails you need to install the package from source.\nUnder Windows this requires to install Rtools\n(https://cran.r-project.org/bin/windows/Rtools/) and to add it to the PATH\nvariable. After that you can run the following lines of code:\n\n```\ndownload.file(\"https://support.gams.com/_media/gdxrrw:gdxrrw_1.0.2.tar.gz\",\n              \"gdxrrw_1.0.2.tar.gz\")\ninstall.packages(\"gdxrrw_1.0.2.tar.gz\",repos = NULL, type=\"source\")\n```\n\n--------------------------------------------------------------------------------\n\n\nAfter that all remaining packages can be installed via `install.packages`\n\n```\npkgs <- c(\"ggplot2\",\n          \"curl\",\n          \"gdx\",\n          \"magclass\",\n          \"madrat\",\n          \"mip\",\n          \"lucode\",\n          \"magpie4\",\n          \"magpiesets\",\n          \"lusweave\",\n          \"luscale\",\n          \"goxygen\",\n\t\t  \"luplot\")\ninstall.packages(pkgs)\n```\nFor post-processing model outputs *Latex* is required\n(https://www.latex-project.org/get/). To be seen by the model it also needs to\nadded to the PATH variable of your system.\n\n## HOW TO CONFIGURE\nModel run settings are set in `config/default.cfg` (or another config file of\nthe same structure). New model scenarios can be created by adding a column to\n`config/scenario_config.csv`\n\n## HOW TO RUN\nTo run the model execute `Rscript start.R` (or `source(\"start.R\")` from within\nR) in the main folder of the model.\nThis will give you a list of available run scripts you can choose from. You can\nalso add your own run scripts by saving them in the folder scripts/start. To run\na single model run with settings as stated in default.cfg you can choose start\nscript \"default\". Make sure that the config file has been set correctly before\nstarting the model.\n\n## HOW TO CONTRIBUTE\nWe are interested in working with you! Just contact us through GitHub\n(https://github.com/magpiemodel) or by mail (magpie@pik-potsdam.de) if you have\nfound and/or fixed a bug, developed a new model feature, have ideas for further\nmodel development, suggestions for improvements or anything else. We are open to\nany kind of contribution. Our aim is to develop an open, transparent and\nmeaningful model of the agricultural land use sector to get a better\nunderstanding of the underlying processes and possible futures. Join us doing\nso!\n\n## DEPENDENCIES\nModel dependencies **must be publicly available** and should be Open Source.\nDevelopment aim is to rather minimize than expand dependencies on non-free\nand/or non open source software. That means that besides currently existing\ndependencies on GAMS, the GDXRRW R package and the corresponding solvers there\nshould be no additional dependencies of this kind and that these existing\ndependencies should be resolved in the future if possible.\n\nIf a new R package is added as dependency this package should fulfill the\nfollowing requirements:\n* The package is published under an Open Source license\n* The package is distributed through CRAN or PIK-CRAN (the PIK-based,\n  but publicly available package repository).\n* The package source code is available through a public, version controlled\n  repository such as GitHub\n\nFor other dependencies comparable measures should apply. When a dependency is\nadded this dependency should be added to the *HOW TO INSTALL* section in the\nREADME file of the model framework (mentioning the depencendy and explaining\nhow it can be installed). If not all requirements can be fulfilled by the new\ndependency this case should be discussed with the model maintainer\n(magpie@pik-potsdam.de) to find a good solution for it.\n\n## INPUT DATA\n\nIn order to allow other researchers to reproduce and use work done with MAgPIE\none needs to make sure that all components necessary to perform a run can be\nshared. One of these components is the input data. As proprietary data usually\ndoes not allow its free distribution it should generally be avoided.\n\nWhen adding a new data source, make sure that it can be freely shared with\nothers. If this is not the case please consider using a different source or\nsolution.\n\nData preparation should ideally be performed with the **madrat** data processing\nframework (https://github.com/pik-piam/madrat). This makes sure that the\nprocessing is reproducible and links properly to the already existing data\nprocessing for MAgPIE.\n\nIn case that these recommendations can not be followed we would be happy if you\ncould discuss that issue with the MAgPIE development team\n(magpie@pik-potsdam.de).\n\n## CONTACT\nmagpie@pik-potsdam.de\n\n## KNOWN BUGS\n\n## TROUBLESHOOTING\nPlease contact magpie@pik-potsdam.de\n\n## CITATION\nSee file CITATION.cff or the documentation of the model for information how\nto cite the model.\n\n[![DOI](https://zenodo.org/badge/135430060.svg)](https://zenodo.org/badge/latestdoi/135430060)\n\n## AUTHORS\nSee list of authors in CITATION.cff\n\n## CHANGELOG\nSee log on GitHub (https://github.com/magpiemodel)\n"}
{"url": "https://github.com/amnmalik/becomingAscientist", "owner": "amnmalik", "repository_name": "becomingAscientist", "date_all_variable_collection": "2023-07-04", "description": "Contains notes, ideas, and opinions on the path to becoming a \"proper\" scientist", "size": 4, "stargazers_count": 0, "watchers_count": 0, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "amnmalik", "contributions": 3}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": []}
{"url": "https://github.com/amnmalik/CarbonMarketsProject", "owner": "amnmalik", "repository_name": "CarbonMarketsProject", "date_all_variable_collection": "2023-07-04", "description": "Code for the carbon markets project ", "size": 984, "stargazers_count": 1, "watchers_count": 1, "language": "R", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 1, "default_branch": "master", "contributors": [{"contributor": "amnmalik", "contributions": 39}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 7814}], "readme": "# CarbonMarketsProject\nCode for the carbon markets project \n"}
{"url": "https://github.com/amnmalik/gcam-v6-ceew", "owner": "amnmalik", "repository_name": "gcam-v6-ceew", "date_all_variable_collection": "2023-07-04", "description": null, "size": 130442, "stargazers_count": 0, "watchers_count": 0, "language": "R", "has_issues": true, "has_projects": false, "has_downloads": true, "has_wiki": false, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "amnmalik", "contributions": 163}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 5952973}, {"language": "Python", "num_chars": 12299}, {"language": "Batchfile", "num_chars": 1117}]}
{"url": "https://github.com/amnmalik/gcamv7-ceew", "owner": "amnmalik", "repository_name": "gcamv7-ceew", "date_all_variable_collection": "2023-07-04", "description": "Changes to stock GCAM v7.0 for India-specific analysis", "size": 13426, "stargazers_count": 0, "watchers_count": 0, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": false, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "amnmalik", "contributions": 13}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": []}
{"url": "https://github.com/amnmalik/localrepo", "owner": "amnmalik", "repository_name": "localrepo", "date_all_variable_collection": "2023-07-04", "description": "local files on github", "size": 296, "stargazers_count": 0, "watchers_count": 0, "language": "Batchfile", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "amnmalik", "contributions": 2}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "Batchfile", "num_chars": 1117}], "readme": "# localrepo\nlocal files on github\n"}
{"url": "https://github.com/amnmalik/Master-thesis", "owner": "amnmalik", "repository_name": "Master-thesis", "date_all_variable_collection": "2023-07-04", "description": "Master thesis on 'Kinetics of Olivine Dissolution in Column Experiments' ", "size": 15485, "stargazers_count": 1, "watchers_count": 1, "language": "TeX", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 1, "default_branch": "master", "contributors": [{"contributor": "amnmalik", "contributions": 6}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "TeX", "num_chars": 189288}], "readme": "# Master-thesis\nMaster thesis, written in Latex, on 'Kinetics of Olivine Dissolution in Column Experiments'. Also contains Tableau workbooks for charts drawn on the data-visualization software *Tableau*.\n\n## Usage\nCan serve as a useful template to writing thesis involving chemical equations and laboratory experiments. \n\n### Steps\n1. Open the file `thesis.tex` in a TEX editor.\n2. Compile using XeLaTeX\n\n## Description\nIn January 2017, I submitted my master thesis, as part of the Master program - Integrated Climate System Sciences at the University of Hamburg. My thesis consisted of laboratory experiments conducted for over a year to find the reaction rate of a rock called olivine. I plotted the time-series data using *Tableau* (they provide a free one year student license), made SVG figures using another tool (don't remember the name as of now).\n\n## Abstract\n\nThe application of powdered silicate rock on land to absorb CO2 requires knowledge of the rate of the reaction and factors affecting it. Rate measured using mixed-flow reactors and batch reactors maybe be inaccurate for this purpose because of the low rock to volume ratio, pre-treatment of grains, and short run time. This work fulfills the gap by conducting batch-type column experiments with olivine over a long time 1 y. The results indicate the system has yet to achieve steady-state. The coarse grain-type have higher rate than fine-type and rates differ depending on the reaction product; these are several orders of magnitude smaller than past experiments. Surface-limited rate control or experimental errors cannot explain these results. A theoretical rate model for a packed bed of forsterite is used taking variables from the experiment. The similarity of model and experimental results indicate that the system is predominantly transport-controlled. The results might be relevant in understanding results from column experiments in general and the discrepancy between field and laboratory weathering rates.\n\n"}
{"url": "https://github.com/amnmalik/PhdDefense", "owner": "amnmalik", "repository_name": "PhdDefense", "date_all_variable_collection": "2023-07-04", "description": "Contains code, figures, and presentation for my PhD Defense", "size": 22305, "stargazers_count": 0, "watchers_count": 0, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "amnmalik", "contributions": 7}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [], "readme": "# Phd Defense\nContains code, figures, and presentation for my PhD Defense at the Technical University of Berlin on 29 March 2022.\n[Direct download PDF](malikamanDefense_v4_2022_03_26.pdf)\n"}
{"url": "https://github.com/amnmalik/PhdSummary", "owner": "amnmalik", "repository_name": "PhdSummary", "date_all_variable_collection": "2023-07-04", "description": "Contains all output - published papers, presentations, code during the course of my PhD", "size": 14818, "stargazers_count": 0, "watchers_count": 0, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "amnmalik", "contributions": 21}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [], "readme": "# Description\nContains all output - published papers, presentations, code during the course of my PhD\n## Published Papers (First-author), data, and code\n1. Malik, A., Bertram, C., Despres, J., Emmerling, J., Fujimori, S., Garg, A., Kriegler, E., Luderer, G., Mathur, R., Roelfsema, M., Shekhar, S., Vishwanathan, S., & Vrontisi, Z. (2020). Reducing stranded assets through early action in the Indian power sector. Environmental Research Letters, 15(9), 094091. https://doi.org/10.1088/1748-9326/ab8033 [[Github Code](https://github.com/amnmalik/reducingstrandedassets)]\n\n2.  Malik, A., Bertram, C., Kriegler, E., & Luderer, G. (2021). Climate policy accelerates structural changes in energy employment. Energy Policy, 159, 112642. https://doi.org/10.1016/j.enpol.2021.112642 [[Zenodo Code](https://zenodo.org/record/6035783)]\n\n2. Malik, A., & Bertram, C. (2022). Solar energy as an early just transition opportunity for coal-bearing states in India. Environmental Research Letters. https://doi.org/10.1088/1748-9326/ac5194 [[Zenodo Code](https://zenodo.org/record/5901604)]\n\n## PhD Thesis\n[Barriers to power sector decarbonisation in India](https://github.com/amnmalik/PhDThesis) submitted 5 November 2021 to the Technical University of Berlin\n\n## PhD Defense \n[Defense Presentation](https://github.com/amnmalik/PhdDefense) Held on 29 March 2022 at Faculty III, Process Sciences, at the Technical University of Berlin. Contains code, figures, and slides. \n\n## [Internal Seminars](https://github.com/amnmalik/PhdSummary/tree/master/seminar_presentations)\nThe doctoral program at the Potsdam Institute for Climate Impact Research requires doctoral candidates to present a seminar every six months at their respective departments. The purpose is to update everyone on the progress of the PhD and gain inputs and ideas from other colleagues.\n\n| **Seminar #** | **Title**                                                                        | **Date**          |  \n|---------------|----------------------------------------------------------------------------------|-------------------|\n| 1             | [Post-Paris Policy Analysis in REMIND](seminar_presentations/2018_11_27.pdf)     | 7 May 2018        |  \n| 2             | [Techno-economic and political economy lock-ins in IAMs](seminar_presentations/Malik_Aman_2018_Seminar_red.pdf)| 27 November 2018  |   \n| 3             | [Not Another Coal Story](seminar_presentations/Malik_Aman_2019_05_16.pdf)        | 15 May 2019       |   \n| 4             | [\"Visible and Invisible\" Understanding energy transitions through carbon lock-ins](seminar_presentations/malik_aman_nov2019_v2.pdf) | 7 November 2019   | \n| 5             | [Employment effects of energy transitions](seminar_presentations/malik_aman_september_2020_split.pdf) | 24 September 2020 |   \n| 6             | [Employment considerations in energy transitions](seminar_presentations/Employment_considerations_energy_transition_2021.pdf) | 27 April 2021     |   \n\n## [Posters, Presentations, and talks (External)](https://github.com/amnmalik/PhdSummary/tree/master/posters_talks_presentations)\n| Category          | Title                                    | Event                  | Date             |\n|-------------------|------------------------------------------|------------------------|------------------|\n| Poster            | [Not (yet) another coal story](posters_talks_presentations/Poster_IAMC__en_V8.pdf)  | [IAMC 2018](https://www.iamconsortium.org/event/eleventh-annual-meeting-of-the-iamc-2018/)      | 13 November 2018 |\n| Oral presentation | [Employment effects in energy transitions](posters_talks_presentations/IAMC-final_malik_aman_2020.pdf) | [IAMC 2020](https://www.iamconsortium.org/event/thirteenth-annual-meeting-of-the-iamc-2020/)      | 1 December 2020  |\n| Expert panel      | [Climate Action and Renewable Energy in context of a Green Recovery](posters_talks_presentations/malik_aman_pik_2020_09_28.pdf)  |  Africa- EU Climate Partnership               | 28 Sepember 2020  |\n| Presentation      |[ Preventing stranded assets and empowering renewables](posters_talks_presentations/Malik_Aman.pdf ) | [CD-LINKS Summer school](https://www.cd-links.org/?p=1816) |    July 2019              |\n| Presentation  |  Preparation and selection of global scenarios in the context of the global stocktake   |    [COMMIT project meeting](https://themasites.pbl.nl/commit/)                    |    12-14 September 2018              |\n"}
{"url": "https://github.com/amnmalik/PhDThesis", "owner": "amnmalik", "repository_name": "PhDThesis", "date_all_variable_collection": "2023-07-04", "description": "Contains code and figures to my PhD thesis", "size": 12999, "stargazers_count": 0, "watchers_count": 0, "language": "TeX", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "amnmalik", "contributions": 19}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": ["climate-change", "coal", "energy", "energy-transition", "india", "phd", "phd-thesis"], "languages": [{"language": "TeX", "num_chars": 139407}], "readme": "## PhDThesis\nContains code and figures to my PhD thesis titled \"Barriers to power sector decarbonisation in India\".\nThesis submitted to Technical University of Berlin, November 2021\n\n## Download\nTo simply download the thesis (as submitted) in `.pdf`, click [here](thesisUpdated20220401-red.pdf). Note that the uploaded pdf is compressed to save space. To see it in high quality, you must download and run the latex code. To see and run the code online, see the thesis on [Overleaf](https://www.overleaf.com/read/nyqmtrgpkcdq). To see the core chapters of the thesis, based on published research articles, click [here](chapters/pdfs). To download directly from the online repository of doctoral disserations of TU Berlin , click [here](https://depositonce.tu-berlin.de/items/25cc825f-904f-4403-a092-7d6190f92149)\n\n## Usage\nThe thesis is written in latex and uses Xelatex to make the PDF.\n\n## Abstract\nThe role of developing countries like India in climate action has undergone a shift in the last five to ten years. Several factors have led to this development. Firstly, with the signing of the Paris Agreement and its emphasis on bottom-pledges, all countries have become co-enactors to mitigation. Secondly, continued scientific research on co-benefits and climate damages has reduced the gap between mitigation and development priorities. Lastly, capital costs of renewable energy (RE) have plummeted making them cheaper than new coal plants in most countries, thereby providing a solid economic incentive to increase the share of RE. Despite these developments, decarbonisation of the power sector in low-income countries faces significant socio-economic and political barriers. This dissertation identifies some of those barriers, eventually suggesting policy solutions to overcome them. While one publication of this cumulative dissertation % I changed it here\nhas a global scope, the other two papers focus on India, a country with low cumulative historic emissions, but is currently the third-largest emitter of greenhouse gases (GHG). Per-capita energy consumption is still low, but it has one of the fastest growing electricity markets in the world. Thus, the policy decisions in the power sector in India can substantially affect the global goal to decarbonisation.\n\nThe first publication identifies the risk of carbon lock-ins in the power sector if India were to continue a trajectory based on current policies. We find that continued investment into fossils could eventually lead to stranded assets in the future because of the faster pace of decarbonisation required in scenarios achieving the Paris Agreement goals. Since most of the stranding arises from plants yet to be built, it can be avoided through additional capacity installations of RE, i.e., increasing current ambition in RE-deployment and limiting new coal power plants to those under construction. Most of the additional capacity would come from solar and wind, given their large resource potentials and favourable economic viability in India. The expansion potential of other sources like gas, nuclear, and hydro remains low, owing to constraints on supply, cost, and construction duration.\n\nThe second article uses different mitigation scenarios and analyses, on a global level but based on country-specific data, the labor market implications of a decarbonisation policies. Although ambitious policies supporting RE and discouraging coal power, e.g., through a coal moratorium, discussed above are favourable for (future) deep decarbonisation, they could lead to disruptive changes adversely affecting the employment situation, specifically the drastic losses in the fossil sector. We show that in the near-term, stringent mitigation results in a net increase in jobs compared to a weaker climate action scenario (based on currently pledged country objectives), mainly through gains in solar and wind jobs in construction, installation, and manufacturing, despite significantly higher losses in coal fuel supply. However, global energy jobs eventually peak, because the falling labour intensity (i.e. jobs per megawatt, due to increasing productivity) outpace increases in RE installations. In the future, total jobs are still higher in stringent mitigation  than in a scenario with less mitigation with most people employed in the operation and maintenance of RE infrastructure, unlike fuel extraction today.  Although stricter mitigation could lead to higher jobs globally, the role of employment in decarbonisation in specific regions could play out very differently. In countries with significant people employed in fossil-fuel industries, a just transition for those workers could become important. \n\nThe third publication highlights that the regional mismatch of energy infrastructure in India could become a significant barrier to effective decarbonisation. Most of the coal mines and coal power plants in India are concentrated in the poorer eastern states of Chhattisgarh, Odisha, and Jharkhand, where it is an important source of both employment and public economy. On the other hand, the best RE potentials in India are concentrated in the relatively wealthier western and southern states and are home to current and planned RE installations.  Continued fossil investments in coal-bearing regions could widen this gap and in pathways to deep decarbonisation, strongly accelerate the loss of coal jobs. Without complementary opportunities, this would negatively impact the livelihood of people living in these areas. We show that dedicated policies to increase solar installations in coal regions could ensure early geographic diversification of solar energy. It could help build broad support for the energy transition, required for climate targets, and could give India important benefits in terms of avoided climate impacts and local health. At the same time, solar alone cannot provide a just transition and there is an urgent need for engagement with all stakeholders exploring challenges and other opportunities into the transition. \n\nIn summary, despite the proliferation of climate considerations into decision-making at all political levels, there are still significant barriers to decarbonisation. Some of the most pressing challenges for fast-growing economies like India involve avoiding lock-ins in the power sector, which could have far-reaching consequences on the pace and cost of future decarbonisation. Higher-income nations could support the transition by providing cheaper RE-related finance and knowledge of increasing power system flexibility. At the same time, changes in the quantity and structure of jobs in the energy sector could also affect the pace of decarbonisation. Here, one key factor is the just transition of predominantly coal-bearing regions. The regional divide of fossil and RE assets and resources in India means that a regionally balanced transition from a fossil to a RE-based economy would not happen on its own; it needs dedicated policies supporting future solar installations in coal-bearing states. However, given the large size of the current coal workforce, additional solar capacity alone (in these regions) cannot replace all the lost jobs. It therefore requires to look for alternatives beyond the energy sector. \n"}
{"url": "https://github.com/amnmalik/PMGSY-OSM-road-integration", "owner": "amnmalik", "repository_name": "PMGSY-OSM-road-integration", "date_all_variable_collection": "2023-07-04", "description": "Workflow for mapping PMGSY road data into OSM using JOSM.", "size": 10, "stargazers_count": 1, "watchers_count": 1, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 1, "default_branch": "master", "contributors": [{"contributor": "amnmalik", "contributions": 5}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [], "readme": "# Description\nThe Ministry of Rural Development has made its database of roads, habilitations under the [PMGSY rural road connectivity programme](https://pmgsy.nic.in/) available under GODL license which is compatible with OSM data. This means that data from PMGSY can be used to map in OSM.\n\n## Need\nSince there are [various issues](https://github.com/datameet/pmgsy-geosadak/issues/14) which prevent directly importing PMGSY data into OSM, road data from PMGSY can only be mapped manually or semi-automatically. This workflow explains one method utilising PMGSY database as a background layer along with the [MapwithAI](Mapwith.ai/) plugin on JOSM\n\n## Contact\nFor any queries or issues, either start an issue or write to aman.malik@outlook.com\n\n# Prerequisite\nKnowledge of editing OSM using JOSM. For a beginner's guide, see [here](https://learnosm.org/en/josm/)\n\n# Workflow\n## I. Adding PMGSY as background layer\n1. Open JOSM. Download latest version from [here](https://josm.openstreetmap.de/)\n2. Go to `Imagery` -> `Imagery Preferences`. Click on **Add a new WMTS layer by adding a URL** (+ WMTS). A new window opens.\n3. Enter the following URL in column 2: `wmts:https://api.mapbox.com/styles/v1/planemad/cknj1leps0ywv17lrj8d16vnj/wmts?access_token=pk.eyJ1IjoicGxhbmVtYWQiLCJhIjoiY2l2eDhnNzNpMDAwNzJ5cGowcnpiMXJkdyJ9.NljuPglsRA3mTGf-4CLIEg`\n4. Give layer name as PMGSY. Click okay.\n5. The imagery now appears as `PMGSY` when you click on `Imagery`\n6. The imagery looks like this. A thin black line indicates all roads from the PMGSY database. A grey line indicates data from OSM. A red line indicates data not in OSM.  \n![image](https://user-images.githubusercontent.com/43343789/165785056-0920cc67-85b6-4e51-8a63-e593643c06b3.png). \n\n7. For a complete map with all PMGSY data, click [here](https://projects.datameet.org/pmgsy-geosadak/map.html#12.15/31.94553/77.14099/-28.8)\n\n\n## II. Installing MapwithAI plugin on JOSM\n1. Go to `Edit` -> `Preferences`. A new window opens. Scroll down to `Plugins`.  ![image](https://user-images.githubusercontent.com/43343789/165784364-7892d249-fc40-4647-a48f-d6aaa19dffb4.png)\n2. Search for `mapwithai`. Install the latest version (1.9.10 at the time of writing).\n3. Restart JOSM. \n\n## III. Mapping \n1. Go to `Imagery` and click on `PMGSY`. The background layer opens.\n2. Select any region/area you want to map and click on `Download Map Data`![image](https://user-images.githubusercontent.com/43343789/165787718-8c33276d-73e6-4f86-8267-e2d29661db12.png). A new window opens. Click on both Openstreetmap data and MapwithAI data; click `Download` \n![image](https://user-images.githubusercontent.com/43343789/165788879-606d6780-f849-4650-b36e-30215d968c99.png)\n\n3. In the layer window on the right, both `Data Layer` and `Mapwithai` layers should be available. If no `Mapwithai` layer is visible, go to `Data` -> `Mapwithai`-> Download data. A new mapwtihai layer should appear but it is probably empty. \n\n5. Go back to Data->MapWithai-> and click on `MapWithAI`\n![image](https://user-images.githubusercontent.com/43343789/165790313-473ce112-7957-4809-a670-21d5bbe30ccb.png)\n\n6. MapwithAI data appears as a pink line. This data is calculated by running AI algorithms on satellite imagery to find roads. In the example below, you see that the algorithm finds: i) roads marked as missing in OSM but present in PMGSY map (blue arrow), ii) Roads not available in PMGSY but in satellite imagery (black arrow), iii) Roads in PMGSY data but not in MapwithAI (red arrow).\n![image](https://user-images.githubusercontent.com/43343789/165791587-d19deea5-1479-42da-aa70-69561d4355ae.png)\n\n7. Although the PMGSY has lots of previously unmapped roads, the database is not comprehensive, resulting in roads that the AI algorithm finds but are not in the PMGSY database. At the same time, the AI algorithm can often mistake trails, riverbeds, or electricity lines for roads. If you think that the road found by MapwithAI is indeed a road, feel free to mark it. From point iii), we see that the AI algorithm sometimes also fails to find roads. This can be due to many reasons but often is because of tree or vegetation cover. If the road is visible on PMGSY, you can map it by hand.\n\n8. To upload the data found using mapwithai on OSM, you first need to transfer the data into the `Data` layer. To do this click on the pink line and press `Shift + A`. Activating the `Data` layer shows that the line is now visible there.\n![image](https://user-images.githubusercontent.com/43343789/165795600-6b28da5c-6b28-49d2-8fb2-22a496c52b5c.png)\n\n9. The AI algorithm already fills in default values `highway=*` and `source=*` tags for a marked line. You can correct them if you want  before uploading.\n\n\n\n\n"}
{"url": "https://github.com/amnmalik/reducingstrandedassets", "owner": "amnmalik", "repository_name": "reducingstrandedassets", "date_all_variable_collection": "2023-07-04", "description": "Contains data and code to produce figures published in the paper https://iopscience.iop.org/article/10.1088/1748-9326/ab8033#references", "size": 2158, "stargazers_count": 0, "watchers_count": 0, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "amnmalik", "contributions": 7}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [], "readme": "# Description\n\nContains data and code to produce figures published in the paper https://iopscience.iop.org/article/10.1088/1748-9326/ab8033#references\n\n"}
{"url": "https://github.com/amnmalik/test", "owner": "amnmalik", "repository_name": "test", "date_all_variable_collection": "2023-07-04", "description": "testing git", "size": 1, "stargazers_count": 0, "watchers_count": 0, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "amnmalik", "contributions": 5}, {"contributor": "pallavidas29", "contributions": 1}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [], "readme": "# test\ntesting git\n\nI have it now locally\n\n\n\nChanging Aman's file\n\n"}
{"url": "https://github.com/amnmalik/Workflows", "owner": "amnmalik", "repository_name": "Workflows", "date_all_variable_collection": "2023-07-04", "description": "Contains concise information on good workflows on Git, R, and general data projects. Information from various sources", "size": 5, "stargazers_count": 0, "watchers_count": 0, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "amnmalik", "contributions": 3}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [], "readme": "## Description\nThe purpose of this file is to document useful workflows for working with Git, R, and general data analysis projects. The idea is to follow good practice of documentation and workflow that allow easy collaboration and reproducability.\n\n### Git\n - **GitHub first, then RStudio** : Before beginning any project, first make a new repository on github and then clone the repository using RStudio in a fresh directory. This way the remote GitHub repo is configured as the `origin` remote for your local repo and your local `main` branch is now tracking the main on GitHub. \n- **Make local changes, save, commit**: Commit your work often, especially after writing important work of code.\n- **Push your local changes to GitHub, but Pull before**: Before pushing your changes online, make sure to pull your changes. While this might not be drastic if you are the only person working, but on projects with collaborators, pushing and then a `pull request` might lead to `merge conflicts`. Push less often than committing and only if the code chunk represents a substantial improvement over last pushed version.\n- **Confirm the local change propagated to the GitHub remote**. To check if your changes were safely deposited to the remote.\n\n### R markdown files\n- **Keeping .md files**: When writing a .Rmd file on Rstudio and knitting it, the .md file is created as an intermediate step but is not stored; instead a .html file is generated adn stored locally. If Github is the main place where you store your code, saving a .md file might be useful. It additionally gives you a free webpage for your results/documentation. Another option is to add `output: github_document` to the YAML. \n- **Keep things machine- and human-readable, whenever possible**\n\n### R projects\n - If you\u2019re in analysis mode and want a report as a side effect, write an R script. If you\u2019re writing a report with a lot of R code in it, write .Rmd. In either case, render to markdown and/or HTML to communicate with other human beings.\n - Use the `here` package for building file paths, once you require sub-directories. See examples [here](https://github.com/jennybc/here_here#readme)\n - Clean out your workspace and restart R and re-run everything periodically\n- `knitr::knit_exit()` somewhere early in your .Rmd document, either in inline R code or in a chunk. Keep moving it earlier until things work. Now move it down in the document.\n\n### Unconnceted files\n- For a quick, stand-alone document that doesn\u2019t fit neatly into a repository or project (yet), make it a [Gist](https://gist.github.com/).\n\n### File naming and file organisation\nhttps://github.com/datacarpentry/rr-organization1/tree/27883c8fc4cdd4dcc6a8232f1fe5c726e96708a0/slides/naming-slides\nhttps://github.com/datacarpentry/rr-organization1/tree/27883c8fc4cdd4dcc6a8232f1fe5c726e96708a0/slides/organization-slides\n\n### References\n[HappyGituseR](https://happygitwithr.com/existing-github-last.html)\n"}
{"url": "https://github.com/bodirsky/Food_demand_model", "owner": "bodirsky", "repository_name": "Food_demand_model", "date_all_variable_collection": "2023-07-04", "description": "Open source model for long-term food demand projections", "size": 2156, "stargazers_count": 1, "watchers_count": 1, "language": "R", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 2, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "GNU Lesser General Public License v3.0", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 2, "open_issues": 0, "watchers": 1, "default_branch": "master", "contributors": [{"contributor": "bodirsky", "contributions": 12}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 38890}], "readme": "# Food_demand_model\n\nOpen source model for long-term food demand projections\n\nCopyright (C) 2015  Susanne Rolinski and Benjamin Leon Bodirsky\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nand the GNU Lesser General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n<b>Contacts:</b>\n\nSusanne Rolinski and Benjamin Leon Bodirsky\nPotsdam Institute for Climate Impact Research\nP.O. Box 60 12 03\n14412 Potsdam\nGermany\nrolinski@pik-potsdam.de\nbodirsky@pik-potsdam.de\n\nThe original version of the model belongs to the following publication:\n* Bodirsky, Benjamin Leon, Susanne Rolinski, Anne Biewald, Isabelle Weindl, Alexander Popp, and H. Lotze-Campen. 2015. \u201cFood Demand Projections for the 21st Century.\u201d PLoS ONE. doi:10.1371/journal.pone.0139201.\n"}
{"url": "https://github.com/bodirsky/Food_demand_visualisation_tool", "owner": "bodirsky", "repository_name": "Food_demand_visualisation_tool", "date_all_variable_collection": "2023-07-04", "description": "Visualisation tool for food demand projections", "size": 572, "stargazers_count": 0, "watchers_count": 0, "language": "JavaScript", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "bodirsky", "contributions": 2}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "JavaScript", "num_chars": 77277}, {"language": "HTML", "num_chars": 39349}, {"language": "CSS", "num_chars": 10833}]}
{"url": "https://github.com/bodirsky/lucode", "owner": "bodirsky", "repository_name": "lucode", "date_all_variable_collection": "2023-07-04", "description": "R package | Landuse Coding Tools", "size": 198, "stargazers_count": 0, "watchers_count": 0, "language": "R", "has_issues": false, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 3, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "Other", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 3, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "tscheypidi", "contributions": 99}, {"contributor": "atreyasha", "contributions": 36}, {"contributor": "dklein-pik", "contributions": 16}, {"contributor": "giannou", "contributions": 4}, {"contributor": "bodirsky", "contributions": 3}, {"contributor": "k4rst3ns", "contributions": 3}, {"contributor": "0UmfHxcvx5J7JoaOhFSs5mncnisTJJ6q", "contributions": 2}, {"contributor": "mishkos", "contributions": 1}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": true, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "R", "num_chars": 158511}], "readme": "# lucode coding tools\n\n## Purpose and Functionality\n\nA collection of tools which allow to manipulate and analyze code.\n\n## Installation\n\nFor installation of the most recent package version an additional repository can be added in R:\n\n```r\noptions(repos = c(CRAN = \"@CRAN@\", rd3mod_repo = \"http://www.pik-potsdam.de/rd3mod/R/\"))\n```\nThe additional repository can be made availably permanently by adding the line above to a file called `.Rprofile` stored in the home folder of your system (`Sys.glob(\"~\")` in R returns the home directory).\n\nAfter that the most recent version of the package can be installed using `install.packages`:\n\n```r \ninstall.packages(\"lucode\")\n```\n\nPackage updates can be installed using `update.packages` (make sure that the additional repository has been added before running that command):\n\n```r \nupdate.packages()\n```\n\n## Questions / Problems\n\nIn case of questions / problems please contact Jan Dietrich <dietrich@pik-potsdam.de>.\n\n\n## Citation\n\n[![DOI](https://zenodo.org/badge/117548029.svg)](https://zenodo.org/badge/latestdoi/117548029)\n\n\n"}
{"url": "https://github.com/giannou/bareTest", "owner": "giannou", "repository_name": "bareTest", "date_all_variable_collection": "2023-07-04", "description": "Null repo for automated testing", "size": 2, "stargazers_count": 0, "watchers_count": 0, "language": null, "has_issues": false, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "MIT License", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [{"contributor": "atreyasha", "contributions": 1}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": []}
{"url": "https://github.com/giannou/gamstest", "owner": "giannou", "repository_name": "gamstest", "date_all_variable_collection": "2023-07-04", "description": null, "size": 14, "stargazers_count": 0, "watchers_count": 0, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "GNU General Public License v3.0", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "giannou", "contributions": 1}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [], "readme": "# gamstest"}
{"url": "https://github.com/giannou/OPEN-PROM", "owner": "giannou", "repository_name": "OPEN-PROM", "date_all_variable_collection": "2023-07-04", "description": null, "size": 854, "stargazers_count": 0, "watchers_count": 0, "language": "GAMS", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 1, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 1, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 1, "open_issues": 1, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "riazis", "contributions": 242}, {"contributor": "giannou", "contributions": 87}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [{"language": "GAMS", "num_chars": 228401}, {"language": "Rebol", "num_chars": 80}], "readme": "# OPEN-PROM\n\nThis is OPEN-PROM (\"open PROMETHEUS\"). The model is currently under development, the present version \nis based on MENA-EDS ENERGY MODEL v4.0 (c) E3Modelling 2020\n \n"}
{"url": "https://github.com/giannou/test", "owner": "giannou", "repository_name": "test", "date_all_variable_collection": "2023-07-04", "description": null, "size": 14, "stargazers_count": 0, "watchers_count": 0, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": "GNU General Public License v3.0", "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "giannou", "contributions": 1}], "howfairis": {"howfairis_repository": true, "howfairis_license": true, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [], "readme": "# test"}
{"url": "https://github.com/giannou/testrepo", "owner": "giannou", "repository_name": "testrepo", "date_all_variable_collection": "2023-07-04", "description": null, "size": 0, "stargazers_count": 0, "watchers_count": 0, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "giannou", "contributions": 1}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [], "readme": "# testrepo"}
{"url": "https://github.com/giannou/testrepo2", "owner": "giannou", "repository_name": "testrepo2", "date_all_variable_collection": "2023-07-04", "description": null, "size": 0, "stargazers_count": 0, "watchers_count": 0, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "main", "contributors": [{"contributor": "giannou", "contributions": 1}], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": [], "readme": "# testrepo"}
{"url": "https://github.com/giannou/trefoil", "owner": "giannou", "repository_name": "trefoil", "date_all_variable_collection": "2023-07-04", "description": null, "size": 0, "stargazers_count": 0, "watchers_count": 0, "language": null, "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "has_discussions": false, "forks_count": 0, "mirror_url": null, "archived": false, "disabled": false, "open_issues_count": 0, "license": null, "allow_forking": true, "is_template": false, "web_commit_signoff_required": false, "visibility": "public", "forks": 0, "open_issues": 0, "watchers": 0, "default_branch": "master", "contributors": [], "howfairis": {"howfairis_repository": true, "howfairis_license": false, "howfairis_registry": false, "howfairis_citation": false, "howfairis_checklist": false}, "topics": [], "languages": []}
